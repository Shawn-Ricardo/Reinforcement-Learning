{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control - Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "This notebook will implement the DDPG algorithm defined in Lillicrap, et al, [\"Continous Control With Deep Reinforcement Learning (2016)\"](https://arxiv.org/pdf/1509.02971.pdf), which aims to train an reinforcement learning agent in environments with continous values for both state and action spaces.\n",
    "\n",
    "Up to this point, we have seen environments with discrete state/action spaces, or with continuous state spaces and discrete action spaces. Being able to train an agent on such an environment holds great potential for real-world applications, such as robotics. \n",
    "\n",
    "This approach is heavily inspired by the DQN algorithm presented by Mnih, et al., in [\"Human Level Control Through Deep Reinforcement Learning (2015)\"](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf). If you are unfamiliar with this algorithm, please refer to the detailed explanation/implementation elsewhere in my github. \n",
    "\n",
    "One restriction of the DQN algorithm is that is it must operate on discrete action spaces. In order to apply the algorithm to environments with continous action spaces, Lillicrap, et al. introduced DPG and an actor-critic architecture. \n",
    "\n",
    "The following cells explain this architecture in detail.\n",
    "\n",
    "\n",
    "### Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import OUNoise, ReplayBuffer\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load environment for a single agent and obtain default brain.\n",
    "if you are unfamiliar with Unity Machine Learning Agents, please refer to the documentation at\n",
    "https://unity3d.com/machine-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"..\\p2_continuous_control\\Reacher_Windows\\Reacher.exe\", no_graphics=False)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of the agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n",
      "States have length: 33\n"
     ]
    }
   ],
   "source": [
    "# environment information\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "n_agents = len(env_info.agents)\n",
    "print('Number of agents:', n_agents)\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU available, take advantage of acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  The Actor\n",
    "\n",
    "DDPG deviates from the \"normal\" actor-critic architecture in several ones. One of those ways is the implementation of the actor. In DDPG, the actor implements a deterministic policy, in that the actor strives to learn the action that will produce the largest expected return. \n",
    "\n",
    "Usually, an actor implements a stochasitc policy by returning a probability distribution over the action space.\n",
    "\n",
    "The paper trained the agent on both pixel data and lower-dimensional data, similar to the state vector in this environment. The authors used 400 and 300 units in the hidden layers for both actor and critic.\n",
    "\n",
    "Lastly, in order to limit the output of the action space, the tanh function is used. This function returns values between -1 and 1. This is a key feature of operating within the continous action space. Rather than produce a single integer value corresponding to a discrete action - i.e., 1 for \"turn left\" - the actor produces values within a range. In this case, that range corresponds to the amount of torque applied to each joint of the arm.\n",
    "\n",
    "Note: The paper uses 400/300 units for hidden layers. In testing, this was unncessary for this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\" \n",
    "    \n",
    "    Actor Model will approximate the policy function, which maps states to actions. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64):\n",
    "        \"\"\"\n",
    "        Initialize parameters and build model.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            \n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        # randomly initilze weights via normal distribution\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        \n",
    "        Build an actor (policy) network that maps states -> actions.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # rectified linear units are used as activation functions\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # the output of the network will be values between -1 and 1. This corresponds to the action space range\n",
    "        # of our unity environment.\n",
    "        return torch.tanh(self.fc3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Critic\n",
    "\n",
    "Again, the DDPG algorithm deviates from normal actor-critic method in the implementation of the critic. \n",
    "\n",
    "Usually, the critic functions as a baseline for the actor, which reduces the variance that is inherent in the actors pure policy pursuit. \n",
    "\n",
    "This variance is still reduced, however, but through different means. Much like the original DQN algorithm, the actor approximates the value function, Q(s,a), by estimating TD rewards. This approach introduces a bias to the system, but also is used to reduce the variance of the actor, since neural network function approximation using estimated returns inherently has low variance and higher bias. \n",
    "\n",
    "Note: The paper uses 400/300 units for hidden layers. In testing, this was unncessary for this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    Critic Model will approximate the value function, which maps a (state, action) pair to an expected reward.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=128, fc2_units=64):\n",
    "        \"\"\"\n",
    "        \n",
    "        Initialize parameters and build model.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        \n",
    "        # The paper concatenates the action values to the output of the first layer for layer 2.\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        \n",
    "        # according to the paper, actions are not included until the second hidden layer.\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent\n",
    "\n",
    "The authors state that network performance was very unstable through many environments when directly implementing Q learning. This behavior was also seen by Mnih, et al, who used Fixed Q-Targets to stabilize this divergent behavior. The authors also implemented the same tactic here by using a local and target network for both the actor and critic. \n",
    "\n",
    "Unlike DQN, the authors reduced the rate at which the target networks \"learned\". In DQN, a 1:1 copying of network paramters is performed as some arbitrary interval. In this approach, the authors implement a soft-update. Gradually moving local network parameters toward target parameters, which greatly stabilized the learning rate.\n",
    "\n",
    "As with DQN, the authors chose to implement a Replay Buffer. This Replay Buffer is identical to that seen in DQN. Please refer to that implementation (located in this github) for a detailed explanation. \n",
    "\n",
    "Lastly, in order to promote exploration within the action space, the authors introduced an Ornstein_Uhlenbeck process which generates temporally correlated exploration for exploration efficiency in physical control problems with inertia. Since this algorithm has existed for quite some time, no explanation will be given. \n",
    "\n",
    "This noise is added to the output of the actor.\n",
    "\n",
    "Note: the agent implementation is very similar to the DQN algorithm. The major change occurs in the learn() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():        \n",
    "    \n",
    "    # most of these paramters were taken from the paper. \n",
    "    # However, some fine tuning was needed for this Unity environment.\n",
    "    \n",
    "    def __init__(self, device, state_size, action_size, n_agents, \n",
    "                 random_seed=np.random.randint(int(1e5)),\n",
    "                 buffer_size=int(1e5), \n",
    "                 batch_size=128, \n",
    "                 gamma=0.99, \n",
    "                 tau=1e-3, \n",
    "                 lr_actor=1e-4, \n",
    "                 lr_critic=1e-4):        \n",
    "\n",
    "        # torch gpu or cpu device\n",
    "        self.device = device\n",
    "        \n",
    "        # environment info\n",
    "        self.state_size = state_size\n",
    "        self.n_agents = n_agents\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # hyperparameters\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.lr_actor = lr_actor\n",
    "        self.lr_critic = lr_critic\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "\n",
    "        # actor network \n",
    "        # target network stabilizes divergence while training.\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=lr_actor)\n",
    "\n",
    "        # critic network \n",
    "        # target network stabilizes divergence while trainig.\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=lr_critic)\n",
    "\n",
    "        # noise process added to actor output. encourages exploration of the action space during training\n",
    "        self.noise = OUNoise((n_agents, action_size), random_seed)\n",
    "\n",
    "        # replay memory, where experience tuples are sampled uniformly\n",
    "        self.memory = ReplayBuffer(device, action_size, buffer_size, batch_size, random_seed)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Save experience in replay memory, and use random sample from buffer to learn.\n",
    "        \"\"\"\n",
    "        \n",
    "        # store experience tuples. \n",
    "        # in cases where multiple agents are running, tuples are indexed per agent\n",
    "        for i in range(self.n_agents):\n",
    "            self.memory.add(state[i,:], action[i,:], reward[i], next_state[i,:], done[i])\n",
    "\n",
    "        \n",
    "        if len(self.memory) > self.batch_size:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"\n",
    "        Returns actions for given state as per current policy.\n",
    "        \"\"\"\n",
    "        \n",
    "        state = torch.from_numpy(state).float().to(self.device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()        \n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "            \n",
    "        # clip, in cases where noise added sets values greater than +/- 1\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"\n",
    "        \n",
    "        Update policy and value parameters using given batch of experience tuples.\n",
    "        \n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        \n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        \n",
    "        Please read this notebook's readme for a more detailed explanation.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        # Clip gradients to avoid large deviations; stabilizes learning.\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target)\n",
    "        self.soft_update(self.actor_local, self.actor_target)\n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"\n",
    "        Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent\n",
    "\n",
    "This training cell is identical to the other training cells seen elsewhere in my github. For a detailed explanation, see other reinforcement learning notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: \t253 \tScore: \t37.72 \tAverage Score: \t30.00\n",
      "Environment solved in 253 episodes!\tAverage Score: 30.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ddpg_train():\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    n_episodes = 1000\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]            # reset the environment\n",
    "        states = env_info.vector_observations\n",
    "        agent.reset()                                                # reset the agent noise\n",
    "        score = np.zeros(n_agents)\n",
    "        \n",
    "        while True:\n",
    "            actions = agent.act(states)\n",
    "        \n",
    "            env_info = env.step(\n",
    "                 actions)[brain_name]               # send the action to the environment                            \n",
    "            next_states = env_info.vector_observations               # get the next state        \n",
    "            rewards = env_info.rewards                               # get the reward        \n",
    "            dones = env_info.local_done                              # see if episode has finished        \n",
    "\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "            score += rewards                                         # update the score\n",
    "        \n",
    "            states = next_states                                     # roll over the state to next time step        \n",
    "                                                        \n",
    "            if np.any(dones):                                          # exit loop if episode finished        \n",
    "                break                                        \n",
    "\n",
    "        scores.append(np.mean(score))\n",
    "        scores_window.append(np.mean(score))\n",
    "\n",
    "        print('\\rEpisode: \\t{} \\tScore: \\t{:.2f} \\tAverage Score: \\t{:.2f}'.format(episode, np.mean(score), np.mean(scores_window)), end=\"\")  \n",
    "        \n",
    "        if np.mean(scores_window) >= 30.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_window)))\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')      \n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            break    \n",
    "\n",
    "    plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()    \n",
    "\n",
    "# create agent\n",
    "agent = Agent(device, state_size, action_size, n_agents)\n",
    "\n",
    "# train the agent\n",
    "ddpg_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch the Smart Agent Track A Target!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: \t0 \tScore: \t39.57\n",
      "Episode: \t1 \tScore: \t38.55\n",
      "Episode: \t2 \tScore: \t39.55\n"
     ]
    }
   ],
   "source": [
    "# test the trained agent\n",
    "agent_test = Agent(device, state_size, action_size, n_agents)\n",
    "agent_test.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "agent_test.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "\n",
    "for episode in range(3):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]        \n",
    "    states = env_info.vector_observations       \n",
    "    score = np.zeros(n_agents)               \n",
    "    \n",
    "    while True:\n",
    "        actions = agent_test.act(states, add_noise=False)                    \n",
    "        env_info = env.step(actions)[brain_name]        \n",
    "        next_states = env_info.vector_observations     \n",
    "        rewards = env_info.rewards       \n",
    "        dones = env_info.local_done\n",
    "        score += rewards\n",
    "        states = next_states\n",
    "\n",
    "        if np.any(dones):                              \n",
    "            break\n",
    "\n",
    "    print('Episode: \\t{} \\tScore: \\t{:.2f}'.format(episode, np.mean(score)))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "\n",
    "Enhancements that can be applied to this approach is Prioritized Experience Replay. Being able to identify and use experience tuples that have the greatest potential for learning has been shown to rapidly advance learning of an agent, as well as produce a higher quality policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
