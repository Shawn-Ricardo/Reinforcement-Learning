{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network Applied to OpenAI Gym's Lunar Lander\n",
    "\n",
    "This notebook will implement a variation of the Deep Q-Network described in DeepMind's [Human-level Control Through Deep Reinforcement Learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf).\n",
    "\n",
    "\n",
    "### Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#!python -m pip install jdc\n",
    "import jdc\n",
    "\n",
    "#!python -m pip install pyvirtualdisplay\n",
    "#from pyvirtualdisplay import Display\n",
    "#display = Display(visible=0, size=(1400, 900)).start()\n",
    "#display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Environment\n",
    "\n",
    "Instantiate the [Lunar Lander Environment](https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py) and print the State and Action space.\n",
    "\n",
    "For more detailed description on the state vector and the environment in general, please visit the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (8,)\n",
      "Number of actions:  4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if GPU available, take advantage of acceleration\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Network\n",
    "\n",
    "The following cell implements the deep neural network that will be used to approximate the non-linear action-value function for this Lunar Lander environment. \n",
    "\n",
    "In the paper, the authors implemented a CNN because they defined the input space to be 84 x 84 x 4. This input space consisted of sequential images from the screen, allowing the agent to take advantage of spatial and temporal relationships that a CNN affords.\n",
    "\n",
    "In this approach, Lunder Lander does not return an image of a screen, but a 1-dimentional state vector. As such, a fully connected neural network will be utilized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        # fully connected layers. Notice how weight parameters are not being given. I am relying on PyTorch to \n",
    "        # initialize these for me.\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        # output layer\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        # utilize rectified linear units as activation functions for each neuron\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer\n",
    "\n",
    "Mnih et al. implemented a Replay Buffer to address several issues present with standard online Q-Learning applied to this environment:\n",
    "\n",
    "1) Each step of experience is potentially used in several update steps. This experience tuple can be a rare occurance/edge case and thus the agent can learn from these cases more often without having to rely on the hope of experiencing the rare occurance in the environment. \n",
    "\n",
    "2) There are strong correlations between consequtive frames in this environment. That is to say, an action at time step T will affect the state at timestep T+1, which in turn affects the state at timestep T+2, and so on. An agent that relies solely on sequential learning runs the risk of being stuck in a local minimum or divering catastrophically. \n",
    "\n",
    "### Decoupling \n",
    "\n",
    "3) When learning sequentially and on-policy (weights are updating every iteration), the current parameters determine the next data sample that these same parameters are trained on. This is the literal equivalent of aiming at a moving target. Replay Buffers allow a decoupling between the target and the parameters that are being changed to obtain this target when weights are kept fixed over N number of iterations. This concept is known as a Fixed Q-Target.\n",
    "\n",
    "In sum, having a replay buffer than contains prior experiences and learns from these experiences only after a certain number of iterations addresses the most salient issues present when adapting Q-Learning for this application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer. keep N number of the most recent experiences.\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size        \n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   The Agent \n",
    "\n",
    "The following cells define the Agent class, which implements the salient features of the above paper. These include Experience Replay and Fixed Q-Targets. For more information on these concepts, please refer to the paper. Specifically, the 'Methods' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed=0, buffer_size=int(1e5), \n",
    "                 batch_size=64, gamma=0.99, tau=1e-3, learning_rate=5e-4, update_rate=4):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "            buffer_size(int): reply buffer size\n",
    "            batch_size(int): minibatch size\n",
    "            gamma(float): discount factor\n",
    "            tau(float): utilized in the soft update of target parameters\n",
    "            lr(float): learning rate\n",
    "            update_rate(int): defines how often the network weights are updated\n",
    "        \"\"\"\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.learning_rate = learning_rate\n",
    "        self.update_rate = update_rate\n",
    "        \n",
    "        # Q-Network. As described in \"decoupling\" above, weights must be kept frozen for a certain number of iterations.\n",
    "        # This interval is defined by the paramters \"update_rate\". Q-Network implementation uses two networks to \"freeze\"\n",
    "        # weights while new experiences are collected.\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed)\n",
    "        \n",
    "        # Initialize time step for updating every 'update_rate' frequency\n",
    "        self.t_step = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lundar Lander program provides a vector of the current state. The agent accepts this state and inputs the state into the Q-Network, which produces an action. Notice that no learning is done at this point.\n",
    "\n",
    "The following cell describes this how the agent implements this concept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Agent\n",
    "\n",
    "def act(self, state, eps=0.):\n",
    "    \"\"\"Returns actions for given state per current policy.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        state (array_like): current state\n",
    "        eps (float): epsilon, for epsilon-greedy action selection. \n",
    "        \n",
    "    Note\n",
    "    =====\n",
    "    For more information on what an epsilon-greedy action policy is, view the SARSA directory elsewhere in my github.\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert numpy to pyTorch tensor\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "    \n",
    "    # change the network to evaluation mode\n",
    "    self.qnetwork_local.eval()\n",
    "    \n",
    "    # disable gradient calculation in autograd. When disabled, autograd does not store the actions performed\n",
    "    # on the forward pass (which are used for gradient calculation with backward() is called).\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # obtain action values for this state produced by the network\n",
    "        action_values = self.qnetwork_local(state)\n",
    "        \n",
    "    # change mode to training\n",
    "    self.qnetwork_local.train()\n",
    "\n",
    "    # Epsilon-greedy action selection\n",
    "    if random.random() > eps:\n",
    "        return np.argmax(action_values.cpu().data.numpy())\n",
    "    else:\n",
    "        return random.choice(np.arange(self.action_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After receiving the action produced by the Q-Network, the Agent will progress the environment by calling *env.step(action)*. \n",
    "\n",
    "This call will return a new state vector that represents the updated environment, as well as a reward for performing that action.\n",
    "\n",
    "The agent will use this State, Action, Reward, Next_State experience to add to its Replay Buffer. In addition, the Agent will \"learn\" - update Q-Network weights - if the interval defined by 'update_rate' has been hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Agent\n",
    "\n",
    "def step(self, state, action, reward, next_state, done):\n",
    "        \n",
    "    # Save experience in replay memory\n",
    "    self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "    # Learn every 'update_rate' time steps.\n",
    "    self.t_step = (self.t_step + 1) % self.update_rate\n",
    "\n",
    "    if self.t_step == 0:\n",
    "\n",
    "        # If enough samples are available in memory, get random subset and learn\n",
    "        if len(self.memory) > self.batch_size:\n",
    "\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, self.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the learning interval has been reached, the Agent will update weights. The following cell implements the concepts discussed above under the section *Replay Experience* and *Decoupling*. In essence, keeps one set of weights fixed during N number of iterations allows a decoupling of the variable weights in the network and the target Q-value that these weights are attempting to approximate. By decoupling these values, the network does not introduce a moving target. \n",
    "\n",
    "For more information on this concept, please read the paper listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Q-Network\n",
    "\n",
    "Hasselt, et al. (2015) introduced Double Deep Q-Learning Network that aims to address the overestimation present in standard Q-Learning and Deep Q-Network, such as that seen in my implementation of the DQN algorithm in this Github. \n",
    "\n",
    "**The following learn() function from the DQN has been modified to incorporate the Double DQN approach.**\n",
    "\n",
    "In this approach, the local network identifies the action (a') that results in the max q-value for a next state (s').\n",
    "Then, the target network obtains this action and finds the maximum state-action (s', a') q-value. This Q(s',a') value is used to calculate the TD error that the network is trained on. \n",
    "\n",
    "By decoupling the action selection from the max state-action q-value, the network is shown to train faster and perform more effectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Agent\n",
    "def learn(self, experiences, gamma):\n",
    "    \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "        gamma (float): discount factor\n",
    "    \"\"\"\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    \n",
    "    # Obtain the local-network predicted q-values for actions taken on the next_state.\n",
    "        \n",
    "    # change local network to inference mode. The aim here is to have the local network predict q-values for actions, not\n",
    "    # to learn.\n",
    "    self.qnetwork_local.eval()\n",
    "    \n",
    "    # disable gradient calculation in autograd. When disabled, autograd does not store the actions performed\n",
    "    # on the forward pass (which are used for gradient calculation with backward() is called).\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        \"\"\"\n",
    "         obtain action values for this state produced by the network.\n",
    "         the resulting vector is a (bs x 1) in shape, were bs is the batch size.\n",
    "         the scalar values are integers representing the action that produced the maximum q-value.\n",
    "         The argmax() call isolates 1st dimesion and returns the index corresponding to the largest value.\n",
    "         In this case, the 1st dimension is an array of q-values for all actions in the action space.\n",
    "        \"\"\"\n",
    "        next_state_action_values_localNet = self.qnetwork_local(next_states).argmax(1).unsqueeze(1)\n",
    "        \n",
    "        \n",
    "    # change mode to training\n",
    "    self.qnetwork_local.train()\n",
    "    \n",
    "    \"\"\"\n",
    "     Obtain the target-network predicted q-values for actions taken on the next_state.\n",
    "     detach() will produce a new tensor that is separated from the network. \n",
    "     gather() will extract the 1st dimension of the tensor, which correspond to the arrays of action values. \n",
    "     next_state_action_values_localNet identifies what index to obtain from the array of action values.\n",
    "     Again, this index corresponds to the action with the maximum estimated q-value by the local network.\n",
    "    \"\"\"\n",
    "    Q_targets_next = self.qnetwork_target(next_states).detach().gather(1, next_state_action_values_localNet)\n",
    "    \n",
    "    \"\"\"\n",
    "     Compute Q targets for current states. \n",
    "     When done[i] equals 1, the Q_target is the reward. This is consistent behavior with reinforcement learning applications\n",
    "     for the end state.\n",
    "    \n",
    "     At this point, the implementation of DDQN is complete, as the decoupled Q(s,a) has been obtained. \n",
    "    \"\"\"\n",
    "    Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "\n",
    "    # Get expected Q values from local model\n",
    "    \n",
    "    \"\"\"\n",
    "    qnetwork_local(states) returns a (bs x action_size) shaped tensor. For example, result.shape = 64x4, where result[0]\n",
    "    returns an array of length 4 corresponding to the various actions that can be taken in this environment and their\n",
    "    associated q-value. that is, result[0][0] = -0.47 for action ==> 0 ==> 'boost right'.\n",
    "    \n",
    "    gather(1, will operate on the 1st dimension, which corresponds to the arrays themselves.\n",
    "    'actions' is a (bs x 1) shaped vector (vertical vector), where each entry corresponds to the action that was taken. \n",
    "    that is, actions[5] = 3 for action 'boost left'. \n",
    "    gather(1,actions) will filter on the first dimension and pick the index of the array given by the corresponding entry\n",
    "    in the actions vector. this will return a (bs x 1) shaped vector where the scalar value is \n",
    "    the q-value of the action that was taken.\n",
    "    \n",
    "    Notice that detach is not called. so, autograd will record the operations performed in order to calculate gradients.\n",
    "    \"\"\"\n",
    "    Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = F.mse_loss(Q_expected, Q_targets)\n",
    "    \n",
    "    # Minimize the loss\n",
    "    # zero out the autograd so that prior gradient calculations are not incorporated into this pass.\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "    # update TARGET network by setting new weight values within the network.\n",
    "    self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A soft update is performed when updating the values of the target network. Tau specifies the degree to which the new value reflects the original value. This is seen by the multiplication of (tau) and (1-tau) in the update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Agent\n",
    "\n",
    "def soft_update(self, local_model, target_model, tau):\n",
    "    \"\"\"Soft update model parameters.\n",
    "\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        local_model (PyTorch model): weights will be copied from\n",
    "        target_model (PyTorch model): weights will be copied to\n",
    "        tau (float): interpolation parameter \n",
    "    \"\"\"\n",
    "\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "\n",
    "        # the 'underline' character indicates that this action is to be performed on the variable itself,\n",
    "        # rather than on a copy of the variable.\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Train the Agent\n",
    "\n",
    "At this point, all the pieces are present to begin training the Agent on the Lundar Landing environment using a Deep Q-Network.\n",
    "\n",
    "The following cell is very similar in structure to the function seen elsewhere in my github (for Discretization and SARSA) that interacts with the OpenAI Gym environment and the Agent. As such, I won't explain much here, as that has been done in other notebooks within my Github under Reinforcement Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -194.06\n",
      "Episode 200\tAverage Score: -157.02\n",
      "Episode 300\tAverage Score: -56.239\n",
      "Episode 400\tAverage Score: -30.03\n",
      "Episode 500\tAverage Score: 30.111\n",
      "Episode 600\tAverage Score: 134.74\n",
      "Episode 690\tAverage Score: 200.53\n",
      "Environment solved in 590 episodes!\tAverage Score: 200.53\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def deep_Q_learning(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"\n",
    "    Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            \n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                break \n",
    "                \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon. For more information on the importance of epsilon \n",
    "                                          # to an agent's action policy, see the SARSA directory in this Github.\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            \n",
    "        if np.mean(scores_window)>=200.0:\n",
    "            \n",
    "            # a mean score >= 200 represents a solved state. At this point, training has been complete.\n",
    "            # save the network weights for future testing.\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint_ddqn.pth')\n",
    "            break\n",
    "            \n",
    "    return scores\n",
    "\n",
    "agent = Agent(state_size=8, action_size=4, seed=0)\n",
    "scores = deep_Q_learning()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch the Smart Agent Land the Spaceship!\n",
    "\n",
    "The following code will utilize the trained network to successfully land the spaceship within the goal region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the weights from file\n",
    "agent_test = Agent(state_size=8, action_size=4, seed=0)\n",
    "agent_test.qnetwork_local.load_state_dict(torch.load('checkpoint_ddqn.pth'))\n",
    "\n",
    "for i in range(3):\n",
    "    state = env.reset()\n",
    "    img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    for j in range(600):\n",
    "        action = agent_test.act(state)\n",
    "        img.set_data(env.render(mode='rgb_array')) \n",
    "        plt.axis('off')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
