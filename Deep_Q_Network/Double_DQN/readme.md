
In 2015, Hasselt, et al. addressed the overestimation present in Q-Learning in the paper, ["Deep Reinforcement Learning with Double Q-Learning"](https://arxiv.org/pdf/1509.06461.pdf).

Due to Q-Learning including a maximization step over estimated action values, it can learn unrealistically high action values when these values are inaccurate. Moreover, inaccuracies of any kind can induce an upward bias, regardless of whether or not these errors are due to environmental noise, function approximation, non-stationarity, or any other souce. Since imprecise action estimates are the norm during learning, these overestimates are very common. 

The following graph showcases the degree to which overestimation is possible when Q-Learning is applied to function approximation of a continuous state with 10 discrete actions in each state. 


![Alt text](images/dqn_overestimation.PNG)


On the left column, the purple line is the true value. The middle column green line is all the estimated values. The black dashed line is the maximum of these values, where the maximum is much higher than the true value almost everywhere. 

Empirically, Hasselt, et al. showcase Q-Learnings overestimation on Atario 2600 games using the Atari Learning Environment. The following graph demonstrates DQNs over-optimism can be extreme. 


![Alt text](images/atari_graph.PNG)


The graph shows DQN's overestimation in six Atari games, as well as the lesser quality policy that is generated by a DQN. Notice that the blue line is often higher than the orange line. Suggesting that a Double DQN network not only produces more accurate value estimates, but better policies, as well.


![](images/lunar_lander_ddqn.gif)
