
In 2015, Hasselt, et al. addressed the overestimation present in Q-Learning in the paper, ["Deep Reinforcement Learning with Double Q-Learning"](https://arxiv.org/pdf/1509.06461.pdf).

Due to Q-Learning including a maximization step over estimated action values, it can learn unrealistically high action values when these values are inaccurate. Moreover, inaccuracies of any kind can induce an upward bias, regardless of whether or not these errors are due to environmental noise, function approximation, non-stationarity, or any other souce. Since imprecise action estimates are the norm during learning, these overestimates are very common. 

The following graph showcases the degree to which overestimation is possible when Q-Learning is applied to function approximation of a continuous state with 10 discrete actions in each state. 


![Alt text](images/dqn_overestimation.PNG)


On the left column, the purple line is the true value. The middle column green line is all the estimated values. The black dashed line is the maximum of these values, where the maximum is much higher than the true value almost everywhere. 

Empirically, Hasselt, et al. showcase Q-Learnings overestimation on Atario 2600 games using the Atari Learning Environment. The following graph demonstrates DQNs over-optimism can be extreme. 


![Alt text](images/atari_graph.PNG)


The graph shows DQN's overestimation in six Atari games, as well as the lesser quality policy that is generated by a DQN. Notice that the blue line is often higher than the orange line. Suggesting that a Double DQN network not only produces more accurate value estimates, but better policies, as well.

In order to address overestimation of Q-Learning, Hasselt, at el. decomposed the max operation in the target into action selection and action evaluation. Recall that the TD error is calculated using the following equation: 

![Alt text](images/td_target.PNG)

In this approach, the max operator uses the same values both to select and to evaluate an action, which makes it more likely to select overestimated values, resulting in overoptimistic value estimates. 

By decoupling the action selection from the action evaluation, the authors were able to drive down overestimation significantly. Leading to faster learning times, more accurate value estimates, and overall higher quality policies. 

The following graph showcases the learning of an agent on OpenAI Gym's Lunar Lander environment. The graph on the left is the standard DQN network found in this GitHub. The graph on the right is the Double DQN network. Notice the fluctuations in average score for DQN and how the standard DQN took significantly longer to train. Qualitatively, the GIF of the Lundar Lander for Double DQN performs considerably better than the standard DQN.

![Alt text](images/atari_graph.PNG)       ![Alt text](images/atari_graph.PNG)



![](images/lunar_lander_ddqn.gif)
