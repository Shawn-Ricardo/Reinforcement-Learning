{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Agent Navigation\n",
    "\n",
    "\n",
    "This notebook will utilize a Unity ML-Agents environment to train an agent to collect yellow bananas while avoiding blue bananas. [Unity Machine Learning Agents](https://unity3d.com/machine-learning) is an open-source Unity plugin that enables games and simulations to serve as environments for training intelligent agents. Users can train agents on images from this high-fidelity environment, can recieve a state vector (as in this example), or a combination of both.\n",
    "\n",
    "\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Begin by importing necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unityagents in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: pytest>=3.2.2 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from unityagents) (4.0.2)\n",
      "Requirement already satisfied: Pillow>=4.2.1 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from unityagents) (5.3.0)\n",
      "Requirement already satisfied: tensorflow==1.7.1 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from unityagents) (1.7.1)\n",
      "Requirement already satisfied: docopt in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from unityagents) (0.6.2)\n",
      "Requirement already satisfied: grpcio==1.11.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from unityagents) (1.11.0)\n",
      "Requirement already satisfied: protobuf==3.5.2 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from unityagents) (3.5.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from unityagents) (3.13)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from unityagents) (1.15.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from unityagents) (3.0.2)\n",
      "Requirement already satisfied: jupyter in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from unityagents) (1.0.0)\n",
      "Requirement already satisfied: py>=1.5.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from pytest>=3.2.2->unityagents) (1.7.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from pytest>=3.2.2->unityagents) (1.2.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from pytest>=3.2.2->unityagents) (40.6.2)\n",
      "Requirement already satisfied: pluggy>=0.7 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from pytest>=3.2.2->unityagents) (0.8.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from pytest>=3.2.2->unityagents) (4.3.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from pytest>=3.2.2->unityagents) (18.2.0)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from pytest>=3.2.2->unityagents) (0.4.1)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from pytest>=3.2.2->unityagents) (1.12.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from tensorflow==1.7.1->unityagents) (0.6.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from tensorflow==1.7.1->unityagents) (1.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from tensorflow==1.7.1->unityagents) (0.7.1)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from tensorflow==1.7.1->unityagents) (0.32.3)\n",
      "Requirement already satisfied: tensorboard<1.8.0,>=1.7.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from tensorflow==1.7.1->unityagents) (1.7.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from tensorflow==1.7.1->unityagents) (0.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from matplotlib->unityagents) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from matplotlib->unityagents) (2.7.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from matplotlib->unityagents) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from matplotlib->unityagents) (2.3.0)\n",
      "Requirement already satisfied: notebook in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from jupyter->unityagents) (5.7.2)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from jupyter->unityagents) (5.1.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from jupyter->unityagents) (7.4.2)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from jupyter->unityagents) (6.0.0)\n",
      "Requirement already satisfied: qtconsole in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from jupyter->unityagents) (4.4.3)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from jupyter->unityagents) (5.4.0)\n",
      "Requirement already satisfied: html5lib==0.9999999 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.1->unityagents) (0.9999999)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.1->unityagents) (3.0.1)\n",
      "Requirement already satisfied: bleach==1.5.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.1->unityagents) (1.5.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.1->unityagents) (0.14.1)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from notebook->jupyter->unityagents) (0.5.0)\n",
      "Requirement already satisfied: Send2Trash in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from notebook->jupyter->unityagents) (1.5.0)\n",
      "Requirement already satisfied: jupyter-core>=4.4.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from notebook->jupyter->unityagents) (4.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from notebook->jupyter->unityagents) (2.10)\n",
      "Requirement already satisfied: tornado>=4 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from notebook->jupyter->unityagents) (5.1.1)\n",
      "Requirement already satisfied: jupyter-client>=5.2.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from notebook->jupyter->unityagents) (5.2.4)\n",
      "Requirement already satisfied: nbformat in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from notebook->jupyter->unityagents) (4.4.0)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from notebook->jupyter->unityagents) (4.3.2)\n",
      "Requirement already satisfied: terminado>=0.8.1 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from notebook->jupyter->unityagents) (0.8.1)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from notebook->jupyter->unityagents) (0.2.0)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from notebook->jupyter->unityagents) (17.1.2)\n",
      "Requirement already satisfied: ipython>=5.0.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from ipykernel->jupyter->unityagents) (7.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.4.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from ipywidgets->jupyter->unityagents) (3.4.2)\n",
      "Requirement already satisfied: pygments in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from jupyter-console->jupyter->unityagents) (2.3.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from jupyter-console->jupyter->unityagents) (2.0.7)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from nbconvert->jupyter->unityagents) (0.5.0)\n",
      "Requirement already satisfied: mistune>=0.8.1 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from nbconvert->jupyter->unityagents) (0.8.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from nbconvert->jupyter->unityagents) (0.2.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from nbconvert->jupyter->unityagents) (1.4.2)\n",
      "Requirement already satisfied: testpath in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from nbconvert->jupyter->unityagents) (0.4.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from jinja2->notebook->jupyter->unityagents) (1.1.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from nbformat->notebook->jupyter->unityagents) (2.6.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from traitlets>=4.2.1->notebook->jupyter->unityagents) (4.3.0)\n",
      "Requirement already satisfied: pywinpty>=0.5; os_name == \"nt\" in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from terminado>=0.8.1->notebook->jupyter->unityagents) (0.5.5)\n",
      "Requirement already satisfied: backcall in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter->unityagents) (0.1.0)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter->unityagents) (0.13.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter->unityagents) (0.7.5)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from prompt-toolkit<2.1.0,>=2.0.0->jupyter-console->jupyter->unityagents) (0.1.7)\n",
      "Requirement already satisfied: parso>=0.3.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->jupyter->unityagents) (0.3.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jdc in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (0.0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install unityagents\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "!python -m pip install jdc\n",
    "import jdc\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana/Banana.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [0.         1.         0.         0.         0.16895212 0.\n",
      " 1.         0.         0.         0.20073597 1.         0.\n",
      " 0.         0.         0.12865657 0.         1.         0.\n",
      " 0.         0.14938059 1.         0.         0.         0.\n",
      " 0.58185619 0.         1.         0.         0.         0.16089135\n",
      " 0.         1.         0.         0.         0.31775284 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU available, take advantage of acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Deep Q-Network\n",
    "\n",
    "The following cell implements the deep neural network that will be used to approximate the non-linear action-value function for this Banana Collection environment. \n",
    "\n",
    "In the paper, the authors implemented a CNN because they defined the input space to be 84 x 84 x 4. This input space consisted of sequential images from the screen, allowing the agent to take advantage of spatial and temporal relationships that a CNN affords.\n",
    "\n",
    "In this approach, Lunder Lander does not return an image of a screen, but a 1-dimentional state vector. As such, a fully connected neural network will be utilized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        # fully connected layers. Notice how weight parameters are not being given. I am relying on PyTorch to \n",
    "        # initialize these for me.\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        # output layer\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        # utilize rectified linear units as activation functions for each neuron\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Replay Buffer\n",
    "\n",
    "Mnih et al. implemented a Replay Buffer to address several issues present with standard online Q-Learning applied to this environment:\n",
    "\n",
    "1) Each step of experience is potentially used in several update steps. This experience tuple can be a rare occurance/edge case and thus the agent can learn from these cases more often without having to rely on the hope of experiencing the rare occurance in the environment. \n",
    "\n",
    "2) There are strong correlations between consequtive frames in this environment. That is to say, an action at time step T will affect the state at timestep T+1, which in turn affects the state at timestep T+2, and so on. An agent that relies solely on sequential learning runs the risk of being stuck in a local minimum or divering catastrophically. \n",
    "\n",
    "### 5. Decoupling \n",
    "\n",
    "3) When learning sequentially and on-policy (weights are updating every iteration), the current parameters determine the next data sample that these same parameters are trained on. This is the literal equivalent of aiming at a moving target. Replay Buffers allow a decoupling between the target and the parameters that are being changed to obtain this target when weights are kept fixed over N number of iterations. This concept is known as a Fixed Q-Target.\n",
    "\n",
    "In sum, having a replay buffer than contains prior experiences and learns from these experiences only after a certain number of iterations addresses the most salient issues present when adapting Q-Learning for this application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer. keep N number of the most recent experiences.\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size        \n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   6. The Agent \n",
    "\n",
    "The following cells define the Agent class, which implements the salient features of the above paper. These include Experience Replay and Fixed Q-Targets. For more information on these concepts, please refer to the paper. Specifically, the 'Methods' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed=0, buffer_size=int(1e5), \n",
    "                 batch_size=64, gamma=0.99, tau=1e-3, learning_rate=5e-4, update_rate=4):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "            buffer_size(int): reply buffer size\n",
    "            batch_size(int): minibatch size\n",
    "            gamma(float): discount factor\n",
    "            tau(float): utilized in the soft update of target parameters\n",
    "            lr(float): learning rate\n",
    "            update_rate(int): defines how often the network weights are updated\n",
    "        \"\"\"\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.learning_rate = learning_rate\n",
    "        self.update_rate = update_rate\n",
    "        \n",
    "        # Q-Network. As described in \"decoupling\" above, weights must be kept frozen for a certain number of iterations.\n",
    "        # This interval is defined by the paramters \"update_rate\". Q-Network implementation uses two networks to \"freeze\"\n",
    "        # weights while new experiences are collected.\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed)\n",
    "        \n",
    "        # Initialize time step for updating every 'update_rate' frequency\n",
    "        self.t_step = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Banana Collector program provides a vector of the current state. The agent accepts this state and inputs the state into the Q-Network, which produces an action. Notice that no learning is done at this point.\n",
    "\n",
    "The following cell describes this how the agent implements this concept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Agent\n",
    "\n",
    "def act(self, state, eps=0.):\n",
    "    \"\"\"Returns actions for given state per current policy.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        state (array_like): current state\n",
    "        eps (float): epsilon, for epsilon-greedy action selection. \n",
    "        \n",
    "    Note\n",
    "    =====\n",
    "    For more information on what an epsilon-greedy action policy is, view the SARSA directory elsewhere in my github.\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert numpy to pyTorch tensor\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "    \n",
    "    # change the network to evaluation mode\n",
    "    self.qnetwork_local.eval()\n",
    "    \n",
    "    # disable gradient calculation in autograd. When disabled, autograd does not store the actions performed\n",
    "    # on the forward pass (which are used for gradient calculation with backward() is called).\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # obtain action values for this state produced by the network\n",
    "        action_values = self.qnetwork_local(state)\n",
    "        \n",
    "    # change mode to training\n",
    "    self.qnetwork_local.train()\n",
    "\n",
    "    # Epsilon-greedy action selection\n",
    "    if random.random() > eps:\n",
    "        return np.argmax(action_values.cpu().data.numpy())\n",
    "    else:\n",
    "        return random.choice(np.arange(self.action_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After receiving the action produced by the Q-Network, the Agent will progress the environment by calling *env.step(action)*. \n",
    "\n",
    "This call will return a new state vector that represents the updated environment, as well as a reward for performing that action.\n",
    "\n",
    "The agent will use this State, Action, Reward, Next_State experience to add to its Replay Buffer. In addition, the Agent will \"learn\" - update Q-Network weights - if the interval defined by 'update_rate' has been hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Agent\n",
    "\n",
    "def step(self, state, action, reward, next_state, done):\n",
    "        \n",
    "    # Save experience in replay memory\n",
    "    self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "    # Learn every 'update_rate' time steps.\n",
    "    self.t_step = (self.t_step + 1) % self.update_rate\n",
    "\n",
    "    if self.t_step == 0:\n",
    "\n",
    "        # If enough samples are available in memory, get random subset and learn\n",
    "        if len(self.memory) > self.batch_size:\n",
    "\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, self.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the learning interval has been reached, the Agent will update weights. The following cell implements the concepts discussed above under the section *Replay Experience* and *Decoupling*. In essence, keeps one set of weights fixed during N number of iterations allows a decoupling of the variable weights in the network and the target Q-value that these weights are attempting to approximate. By decoupling these values, the network does not introduce a moving target. \n",
    "\n",
    "For more information on this concept, please read the paper listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.Double Q-Network\n",
    "\n",
    "Hasselt, et al. (2015) introduced Double Deep Q-Learning Network that aims to address the overestimation present in standard Q-Learning and Deep Q-Network, such as that seen in my implementation of the DQN algorithm in this Github. \n",
    "\n",
    "**The following learn() function from the DQN has been modified to incorporate the Double DQN approach.**\n",
    "\n",
    "In this approach, the local network identifies the action (a') that results in the max q-value for a next state (s').\n",
    "Then, the target network obtains this action and finds the maximum state-action (s', a') q-value. This Q(s',a') value is used to calculate the TD error that the network is trained on. \n",
    "\n",
    "By decoupling the action selection from the max state-action q-value, the network is shown to train faster and perform more effectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Agent\n",
    "def learn(self, experiences, gamma):\n",
    "    \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "        gamma (float): discount factor\n",
    "    \"\"\"\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    \n",
    "    # Obtain the local-network predicted q-values for actions taken on the next_state.\n",
    "        \n",
    "    # change local network to inference mode. The aim here is to have the local network predict q-values for actions, not\n",
    "    # to learn.\n",
    "    self.qnetwork_local.eval()\n",
    "    \n",
    "    # disable gradient calculation in autograd. When disabled, autograd does not store the actions performed\n",
    "    # on the forward pass (which are used for gradient calculation with backward() is called).\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        \"\"\"\n",
    "         obtain action values for this state produced by the network.\n",
    "         the resulting vector is a (bs x 1) in shape, were bs is the batch size.\n",
    "         the scalar values are integers representing the action that produced the maximum q-value.\n",
    "         The argmax() call isolates 1st dimesion and returns the index corresponding to the largest value.\n",
    "         In this case, the 1st dimension is an array of q-values for all actions in the action space.\n",
    "        \"\"\"\n",
    "        next_state_action_values_localNet = self.qnetwork_local(next_states).argmax(1).unsqueeze(1)\n",
    "        \n",
    "        \n",
    "    # change mode to training\n",
    "    self.qnetwork_local.train()\n",
    "    \n",
    "    \"\"\"\n",
    "     Obtain the target-network predicted q-values for actions taken on the next_state.\n",
    "     detach() will produce a new tensor that is separated from the network. \n",
    "     gather() will extract the 1st dimension of the tensor, which correspond to the arrays of action values. \n",
    "     next_state_action_values_localNet identifies what index to obtain from the array of action values.\n",
    "     Again, this index corresponds to the action with the maximum estimated q-value by the local network.\n",
    "    \"\"\"\n",
    "    Q_targets_next = self.qnetwork_target(next_states).detach().gather(1, next_state_action_values_localNet)\n",
    "    \n",
    "    \"\"\"\n",
    "     Compute Q targets for current states. \n",
    "     When done[i] equals 1, the Q_target is the reward. This is consistent behavior with reinforcement learning applications\n",
    "     for the end state.\n",
    "    \n",
    "     At this point, the implementation of DDQN is complete, as the decoupled Q(s,a) has been obtained. \n",
    "    \"\"\"\n",
    "    Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "\n",
    "    # Get expected Q values from local model\n",
    "    \n",
    "    \"\"\"\n",
    "    qnetwork_local(states) returns a (bs x action_size) shaped tensor. For example, result.shape = 64x4, where result[0]\n",
    "    returns an array of length 4 corresponding to the various actions that can be taken in this environment and their\n",
    "    associated q-value. that is, result[0][0] = -0.47 for action ==> 0 ==> 'boost right'.\n",
    "    \n",
    "    gather(1, will operate on the 1st dimension, which corresponds to the arrays themselves.\n",
    "    'actions' is a (bs x 1) shaped vector (vertical vector), where each entry corresponds to the action that was taken. \n",
    "    that is, actions[5] = 3 for action 'boost left'. \n",
    "    gather(1,actions) will filter on the first dimension and pick the index of the array given by the corresponding entry\n",
    "    in the actions vector. this will return a (bs x 1) shaped vector where the scalar value is \n",
    "    the q-value of the action that was taken.\n",
    "    \n",
    "    Notice that detach is not called. so, autograd will record the operations performed in order to calculate gradients.\n",
    "    \"\"\"\n",
    "    Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = F.mse_loss(Q_expected, Q_targets)\n",
    "    \n",
    "    # Minimize the loss\n",
    "    # zero out the autograd so that prior gradient calculations are not incorporated into this pass.\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "    # update TARGET network by setting new weight values within the network.\n",
    "    self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A soft update is performed when updating the values of the target network. Tau specifies the degree to which the new value reflects the original value. This is seen by the multiplication of (tau) and (1-tau) in the update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Agent\n",
    "\n",
    "def soft_update(self, local_model, target_model, tau):\n",
    "    \"\"\"Soft update model parameters.\n",
    "\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        local_model (PyTorch model): weights will be copied from\n",
    "        target_model (PyTorch model): weights will be copied to\n",
    "        tau (float): interpolation parameter \n",
    "    \"\"\"\n",
    "\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "\n",
    "        # the 'underline' character indicates that this action is to be performed on the variable itself,\n",
    "        # rather than on a copy of the variable.\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  8. Train the Agent\n",
    "\n",
    "At this point, all the pieces are present to begin training the Agent in the Banana Collector environment using a Deep Q-Network.\n",
    "\n",
    "The following cell is very similar in structure to the function seen elsewhere in my github (for Discretization and SARSA) that interacts with the OpenAI Gym environment and the Agent. As such, I won't explain much here, as that has been done in other notebooks within my Github under Reinforcement Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_Q_learning(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"\n",
    "    Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            \n",
    "            action = int(agent.act(state, eps))\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                break \n",
    "        \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon. For more information on the importance of epsilon \n",
    "                                          # to an agent's action policy, see the SARSA directory in this Github.\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            \n",
    "        if np.mean(scores_window)>=15.0:\n",
    "            \n",
    "            # a mean score >= 13 represents a solved state. At this point, training has been complete.\n",
    "            # save the network weights for future testing.\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'banana_collect_ddqn.pth')\n",
    "            break\n",
    "            \n",
    "    return scores\n",
    "\n",
    "agent = Agent(state_size=37, action_size=4, seed=0)\n",
    "scores = deep_Q_learning()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Watch the Smart Agent Collect Bananas!\n",
    "\n",
    "The following code will utilize the trained network to successfully collect yellow bananas, while avoiding blue bananas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 16.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]             # reset the environment\n",
    "state = env_info.vector_observations[0]                        # get the current state\n",
    "score = 0                                                      # initialize the score\n",
    "agent_test = Agent(state_size=37, action_size=4, seed=0)       # initialize smart agent and load trained weights\n",
    "agent_test.qnetwork_local.load_state_dict(torch.load('banana_collect_ddqn.pth'))\n",
    "\n",
    "while True:\n",
    "    \n",
    "    action = int(agent_test.act(state))        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
