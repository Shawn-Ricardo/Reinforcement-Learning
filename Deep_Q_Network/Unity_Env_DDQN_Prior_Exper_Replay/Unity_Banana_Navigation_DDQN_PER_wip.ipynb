{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Agent Navigation\n",
    "\n",
    "\n",
    "This notebook will utilize a Unity ML-Agents environment to train an agent to collect yellow bananas while avoiding blue bananas. [Unity Machine Learning Agents](https://unity3d.com/machine-learning) is an open-source Unity plugin that enables games and simulations to serve as environments for training intelligent agents. Users can train agents on images from this high-fidelity environment, can recieve a state vector (as in this example), or a combination of both.\n",
    "\n",
    "\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Begin by importing necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unityagents in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from unityagents) (1.15.4)\n",
      "Requirement already satisfied: grpcio==1.11.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from unityagents) (1.11.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from unityagents) (3.13)\n",
      "Requirement already satisfied: docopt in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from unityagents) (0.6.2)\n",
      "Requirement already satisfied: pytest>=3.2.2 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from unityagents) (4.0.2)\n",
      "Requirement already satisfied: protobuf==3.5.2 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from unityagents) (3.5.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from unityagents) (3.0.2)\n",
      "Requirement already satisfied: jupyter in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from unityagents) (1.0.0)\n",
      "Requirement already satisfied: Pillow>=4.2.1 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from unityagents) (5.3.0)\n",
      "Requirement already satisfied: tensorflow==1.7.1 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from unityagents) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from grpcio==1.11.0->unityagents) (1.12.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from pytest>=3.2.2->unityagents) (4.3.0)\n",
      "Requirement already satisfied: py>=1.5.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from pytest>=3.2.2->unityagents) (1.7.0)\n",
      "Requirement already satisfied: pluggy>=0.7 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from pytest>=3.2.2->unityagents) (0.8.0)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from pytest>=3.2.2->unityagents) (0.4.1)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from pytest>=3.2.2->unityagents) (1.2.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from pytest>=3.2.2->unityagents) (40.6.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from pytest>=3.2.2->unityagents) (18.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from matplotlib->unityagents) (2.7.5)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from matplotlib->unityagents) (2.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from matplotlib->unityagents) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from matplotlib->unityagents) (1.0.1)\n",
      "Requirement already satisfied: qtconsole in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from jupyter->unityagents) (4.4.3)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from jupyter->unityagents) (5.1.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from jupyter->unityagents) (7.4.2)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from jupyter->unityagents) (5.4.0)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from jupyter->unityagents) (6.0.0)\n",
      "Requirement already satisfied: notebook in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from jupyter->unityagents) (5.7.2)\n",
      "Requirement already satisfied: gast>=0.2.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from tensorflow==1.7.1->unityagents) (0.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from tensorflow==1.7.1->unityagents) (0.32.3)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from tensorflow==1.7.1->unityagents) (0.6.1)\n",
      "Requirement already satisfied: tensorboard<1.8.0,>=1.7.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from tensorflow==1.7.1->unityagents) (1.7.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from tensorflow==1.7.1->unityagents) (1.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from tensorflow==1.7.1->unityagents) (0.7.1)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from qtconsole->jupyter->unityagents) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from qtconsole->jupyter->unityagents) (4.4.0)\n",
      "Requirement already satisfied: traitlets in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from qtconsole->jupyter->unityagents) (4.3.2)\n",
      "Requirement already satisfied: pygments in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from qtconsole->jupyter->unityagents) (2.3.0)\n",
      "Requirement already satisfied: jupyter-client>=4.1 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from qtconsole->jupyter->unityagents) (5.2.4)\n",
      "Requirement already satisfied: ipython>=5.0.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from ipykernel->jupyter->unityagents) (7.2.0)\n",
      "Requirement already satisfied: tornado>=4.2 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from ipykernel->jupyter->unityagents) (5.1.1)\n",
      "Requirement already satisfied: widgetsnbextension~=3.4.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from ipywidgets->jupyter->unityagents) (3.4.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from ipywidgets->jupyter->unityagents) (4.4.0)\n",
      "Requirement already satisfied: mistune>=0.8.1 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from nbconvert->jupyter->unityagents) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from nbconvert->jupyter->unityagents) (1.4.2)\n",
      "Requirement already satisfied: testpath in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from nbconvert->jupyter->unityagents) (0.4.2)\n",
      "Requirement already satisfied: bleach in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from nbconvert->jupyter->unityagents) (1.5.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from nbconvert->jupyter->unityagents) (0.5.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from nbconvert->jupyter->unityagents) (2.10)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from nbconvert->jupyter->unityagents) (0.2.3)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from jupyter-console->jupyter->unityagents) (2.0.7)\n",
      "Requirement already satisfied: terminado>=0.8.1 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from notebook->jupyter->unityagents) (0.8.1)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from notebook->jupyter->unityagents) (17.1.2)\n",
      "Requirement already satisfied: Send2Trash in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from notebook->jupyter->unityagents) (1.5.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from notebook->jupyter->unityagents) (0.5.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.1->unityagents) (3.0.1)\n",
      "Requirement already satisfied: html5lib==0.9999999 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.1->unityagents) (0.9999999)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.1->unityagents) (0.14.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from traitlets->qtconsole->jupyter->unityagents) (4.3.0)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter->unityagents) (0.13.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter->unityagents) (0.7.5)\n",
      "Requirement already satisfied: backcall in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter->unityagents) (0.1.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets->jupyter->unityagents) (2.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from jinja2->nbconvert->jupyter->unityagents) (1.1.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from prompt-toolkit<2.1.0,>=2.0.0->jupyter-console->jupyter->unityagents) (0.1.7)\n",
      "Requirement already satisfied: pywinpty>=0.5; os_name == \"nt\" in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from terminado>=0.8.1->notebook->jupyter->unityagents) (0.5.5)\n",
      "Requirement already satisfied: parso>=0.3.0 in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->jupyter->unityagents) (0.3.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jdc in c:\\users\\shawn\\miniconda3\\envs\\drlnd\\lib\\site-packages (0.0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install unityagents\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "!python -m pip install jdc\n",
    "import jdc\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana/Banana.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU available, take advantage of acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Deep Q-Network\n",
    "\n",
    "The following cell implements the deep neural network that will be used to approximate the non-linear action-value function for this Banana Collection environment. \n",
    "\n",
    "In the paper, the authors implemented a CNN because they defined the input space to be 84 x 84 x 4. This input space consisted of sequential images from the screen, allowing the agent to take advantage of spatial and temporal relationships that a CNN affords.\n",
    "\n",
    "In this approach, Lunder Lander does not return an image of a screen, but a 1-dimentional state vector. As such, a fully connected neural network will be utilized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        # fully connected layers. Notice how weight parameters are not being given. I am relying on PyTorch to \n",
    "        # initialize these for me.\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        # output layer\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        # utilize rectified linear units as activation functions for each neuron\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Replay Buffer\n",
    "\n",
    "Mnih et al. implemented a Replay Buffer to address several issues present with standard online Q-Learning applied to this environment:\n",
    "\n",
    "1) Each step of experience is potentially used in several update steps. This experience tuple can be a rare occurance/edge case and thus the agent can learn from these cases more often without having to rely on the hope of experiencing the rare occurance in the environment. \n",
    "\n",
    "2) There are strong correlations between consequtive frames in this environment. That is to say, an action at time step T will affect the state at timestep T+1, which in turn affects the state at timestep T+2, and so on. An agent that relies solely on sequential learning runs the risk of being stuck in a local minimum or divering catastrophically. \n",
    "\n",
    "### 5. Decoupling \n",
    "\n",
    "3) When learning sequentially and on-policy (weights are updating every iteration), the current parameters determine the next data sample that these same parameters are trained on. This is the literal equivalent of aiming at a moving target. Replay Buffers allow a decoupling between the target and the parameters that are being changed to obtain this target when weights are kept fixed over N number of iterations. This concept is known as a Fixed Q-Target.\n",
    "\n",
    "In sum, having a replay buffer than contains prior experiences and learns from these experiences only after a certain number of iterations addresses the most salient issues present when adapting Q-Learning for this application. \n",
    "\n",
    "### 6. Prioritized Experience Replay\n",
    "\n",
    "In 2016, Schaul et al. introduced Prioritized Experience Replay (PER) in order to prioritize which transitions are replayed. The approach has proven to make experience replay more efficient and effective than if all transitions are replay uniformly; which was the method implemented in Mnih et al.\n",
    "\n",
    "A sumtree is used to implement PER. Please review the readme of this directory to better understand how Schaul et al. utilized this approach.\n",
    "\n",
    "Thank you to [Thomas Simonini's](https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682) wonderful explanation. Please reference his blog, as well as [Patrick Emami](https://pemami4911.github.io/paper-summaries/deep-rl/2016/01/26/prioritizing-experience-replay.html), [JaromÃ­r Janisch](https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/#more-444), and [Quintin Fettes RL_Tutorials](https://github.com/qfettes/DeepRL-Tutorials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    \n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        Initialize a SumTree object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            capacity(int): number of experiences to hold in memory.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # the magnitude of 'capacity' is arbitrary. 'capacity' specifies the number of\n",
    "        # experience tuples to store. Greater 'capacity' => larger memory bank \n",
    "        self.capacity = capacity \n",
    "        \n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        \n",
    "        \"\"\" \n",
    "               [0]\n",
    "             /    \\\n",
    "            /      \\\n",
    "          [1]      [2]\n",
    "         /   \\    /   \\\n",
    "        [3]  [4] [5]  [6]  <--- the leaf nodes contain the priority score for that particular experience.\n",
    "        \n",
    "        \n",
    "        notice that the \"concept\" of the binary tree is being implemented with a numpy array.\n",
    "        \n",
    "        to obtain all the priority scores at once, simply write self.tree[-self.capacity:].\n",
    "        \n",
    "        (2 * capacity - 1) = the number of nodes within the tree. If capacity = 4, the total number\n",
    "        of tree nodes = (2 * 4) - 1 = 7, which is equal to the total number of nodes in the \n",
    "        illustration above.\n",
    "        \"\"\"\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        \n",
    "        # contains all experiences in the memory bank\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "    \n",
    "        \n",
    "        \"\"\"\n",
    "        data_pointer will point to an experience in self.data.\n",
    "\n",
    "        data_pointer will sequentially (from left to right) iterate through [0,capacity-1] and \n",
    "        return to 0 again when (capacity - 1) has been reached. \n",
    "        \n",
    "        the mapping between experiences in 'self.data' and the corresponding leaf-node in 'self.tree' is:\n",
    "        \n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        \n",
    "        So, if data_pointer points to the second element in the self.data array; \n",
    "        data_pointer = 1. \n",
    "        The corresponding leaf node is, tree_index = 1 + 4 - 1 = 4. Thus, self.tree[4] returns the \n",
    "        priority score for the experience at self.data[1].\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_pointer = 0\n",
    "        \n",
    "        \n",
    "    def add(self, priority, data):\n",
    "        \"\"\"\n",
    "        Add the experience tuple into the memory bank and update the priority score at the corresponding\n",
    "        leaf, as well as up the tree.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            priority(float): priority score for this particular experience\n",
    "            data(tuple): contains the experience tuple\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # obtain corresponding tree index\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        \n",
    "        \"\"\" \n",
    "                    0\n",
    "                   / \\\n",
    "                  0   0\n",
    "                 / \\ / \\\n",
    "        tree_index  0 0  0     leaves are filled from left to right\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # replace old experience with  new experience\n",
    "        new_experience = self.experience(state=data[0], action=data[1], reward=data[2], next_state=data[3], done=data[4])\n",
    "        #print('adding experience: ', new_experience)\n",
    "        self.data[self.data_pointer] = new_experience\n",
    "        \n",
    "        # update corresponding leaf node priority value with new value\n",
    "        self.update (tree_index, priority)\n",
    "        \n",
    "        # increment data_point to point to next slot in array\n",
    "        self.data_pointer += 1\n",
    "        \n",
    "        # if data_point has reached the end, start at 0. Note, experiences are overwritten.\n",
    "        if self.data_pointer >= self.capacity:  \n",
    "            self.data_pointer = 0\n",
    "            \n",
    "    \n",
    "    def update(self, tree_index, priority):\n",
    "        \"\"\"\n",
    "        \n",
    "        Update a leaf's priority score and propagate the change through the sumtree.\n",
    "        \n",
    "        Params:\n",
    "        ======\n",
    "           tree_index(int): the index of the leaf node in the tree that will be updated.\n",
    "           priority(float): the value to overwrite a particular leaf node's current value.\n",
    "           \n",
    "        \"\"\"\n",
    "        \n",
    "        # change = new priority score - former priority score.\n",
    "        change = priority - self.tree[tree_index]\n",
    "        \n",
    "        # overwrite current score with the new score.\n",
    "        self.tree[tree_index] = priority\n",
    "        \n",
    "        # propagate the change through tree. This allows for self.tree[0] to always contain the sum\n",
    "        # of the priority scores. \n",
    "        while tree_index != 0: \n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "                   [0]\n",
    "                 /    \\\n",
    "                /      \\\n",
    "              [1]      [2]\n",
    "             /   \\    /   \\\n",
    "            [3]  [4] [5]  [6] \n",
    "            \n",
    "            Suppose tree_index = 6. The goal is to progagate the change in score up the tree.\n",
    "            As such, indices [2] and [0] must be updated. \n",
    "            \n",
    "            The equation is:\n",
    "            tree_index = (tree_index - 1)) // 2\n",
    "            \n",
    "            tree_index = (6 - 1) // 2 = 5 // 2 = 2.5 -> 2\n",
    "            tree_index = (2 - 1) // 2 = 1 // 2 = 0.5 -> 0\n",
    "            (end condition)\n",
    "            \n",
    "            \"\"\"\n",
    "        \n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "    \n",
    "    \n",
    "    def get_leaf(self, v):\n",
    "        \n",
    "        \"\"\"\n",
    "        return the leaf_index, priority score, and experience closest to the value (v) passed in.\n",
    "        \n",
    "        Params:\n",
    "        ======\n",
    "            v(float): a priority score\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        parent_index = 0\n",
    "        leaf_index = 0\n",
    "        gate = True\n",
    "        \n",
    "        # note: I recommend writing the following out paper to get the idea of what it's doing.\n",
    "        while gate: \n",
    "            \n",
    "            # obtain the indices of the children of the parent node\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            \n",
    "            # terminating condition indicating that the leaf node has been reached.\n",
    "            if left_child_index >= len(self.tree):\n",
    "                \n",
    "                leaf_index = parent_index\n",
    "                gate = False\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                # downward search, always search for a higher priority node\n",
    "                # the following link has a SumTree for visualization:\n",
    "                # https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/#more-444\n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                    \n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "        \n",
    "        # obtain corresponding index in self.data array. \n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        \"\"\"\n",
    "        Property declaration. Allows for dot syntax on the object. Returns the sum of priorty scores of\n",
    "        the memory bank.\n",
    "        \n",
    "        Params:\n",
    "        ======\n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "        # root node of a SumTree holds the sum of all nodes within the tree.\n",
    "        return self.tree[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):  # stored as ( s, a, r, s_, done ) in SumTree\n",
    "    \n",
    "    PER_e = 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n",
    "    PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "    PER_b = 0.3  # importance-sampling, from initial value increasing to 1\n",
    "    \n",
    "    PER_b_increment_per_sampling = 0.0000625\n",
    "    \n",
    "    absolute_error_upper = 1.  # clipped abs error\n",
    "    \n",
    "    t_step = 0\n",
    "\n",
    "    def __init__(self, capacity=100000):\n",
    "        # Making the tree \n",
    "        \"\"\"\n",
    "        Remember that our tree is composed of a sum tree that contains the priority scores at his leaf\n",
    "        And also a data array\n",
    "        We don't use deque because it means that at each timestep our experiences change index by one.\n",
    "        We prefer to use a simple array and to overwrite when the memory is full.\n",
    "        \"\"\"\n",
    "        self.tree = SumTree(capacity)\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    Store a new experience in our tree\n",
    "    Each new experience have a score of max_prority (it will be then improved when we use this exp to train our DDQN)\n",
    "    \"\"\"\n",
    "    def store(self, experience):\n",
    "        \n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        #print('max_priority: ', max_priority)\n",
    "        \n",
    "        # If the max priority = 0 we can't put priority = 0 since this exp will never have a chance to be selected\n",
    "        # So we use a minimum priority\n",
    "        #max_priority = int(max_priority * 1000)\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "        \n",
    "        self.tree.add(max_priority, experience)   # set the max p for new p\n",
    "        #self.tree.add(self.absolute_error_upper, experience)\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "    - Then a value is uniformly sampled from each range\n",
    "    - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "    - Then, we calculate IS weights for each minibatch element\n",
    "    \"\"\"\n",
    "    def sample(self, n):\n",
    "        # Create a sample array that will contains the minibatch\n",
    "        memory_b = []\n",
    "        \n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "        \n",
    "        # Calculate the priority segment\n",
    "        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.tree.total_priority / n       # priority segment\n",
    "    \n",
    "    \n",
    "        self.t_step = (self.t_step + 1) % 100\n",
    "        if self.t_step == 0:\n",
    "    \n",
    "            # Here we increasing the PER_b each time we sample a new minibatch\n",
    "            self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])  # max = 1\n",
    "        \n",
    "        # Calculating the max_weight\n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:])\n",
    "        #print('p_min: ', p_min)\n",
    "        \n",
    "        p_min = p_min / self.tree.total_priority\n",
    "        #print('total_priority: ', self.tree.total_priority)\n",
    "        #print('p_min/total_priority: ', p_min)\n",
    "        \n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "        #print('max_weight: ', max_weight)\n",
    "        #print()\n",
    "        \n",
    "        for i in range(n):\n",
    "            \"\"\"\n",
    "            A value is uniformly sample from each range\n",
    "            \"\"\"\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            \n",
    "            \"\"\"\n",
    "            Experience that correspond to each value is retrieved\n",
    "            \"\"\"\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            #P(j)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            \n",
    "            #  IS = (1/N * 1/P(i))**b /max wi == (N*P(i))**-b  /max wi\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight\n",
    "                                   \n",
    "            b_idx[i]= index\n",
    "            \n",
    "            #experience = [data]\n",
    "            \n",
    "            #memory_b.append(experience)\n",
    "            memory_b.append(data)\n",
    "        \n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "    \n",
    "    \"\"\"\n",
    "    Update the priorities on the tree\n",
    "    \"\"\"\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        \n",
    "        abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "        #clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        #ps = np.power(clipped_errors, self.PER_a)\n",
    "        ps = np.power(abs_errors, self.PER_a)\n",
    "        \n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer. keep N number of the most recent experiences.\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size        \n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   6. The Agent \n",
    "\n",
    "The following cells define the Agent class, which implements the salient features of the above paper. These include Experience Replay and Fixed Q-Targets. For more information on these concepts, please refer to the paper. Specifically, the 'Methods' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed=0, buffer_size=int(1e4), \n",
    "                 batch_size=64, gamma=0.99, tau=1e-3, learning_rate=5e-4, update_rate=4):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "            buffer_size(int): reply buffer size\n",
    "            batch_size(int): minibatch size\n",
    "            gamma(float): discount factor\n",
    "            tau(float): utilized in the soft update of target parameters\n",
    "            lr(float): learning rate\n",
    "            update_rate(int): defines how often the network weights are updated\n",
    "        \"\"\"\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.learning_rate = learning_rate\n",
    "        self.update_rate = update_rate\n",
    "        \n",
    "        # Q-Network. As described in \"decoupling\" above, weights must be kept frozen for a certain number of iterations.\n",
    "        # This interval is defined by the paramters \"update_rate\". Q-Network implementation uses two networks to \"freeze\"\n",
    "        # weights while new experiences are collected.\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Replay memory\n",
    "        # comment out the uniform memory bank in favor of the PER\n",
    "        #self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed)\n",
    "        self.memory_per = Memory(buffer_size)\n",
    "        \n",
    "        \n",
    "        # Initialize time step for updating every 'update_rate' frequency\n",
    "        self.t_step = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Banana Collector program provides a vector of the current state. The agent accepts this state and inputs the state into the Q-Network, which produces an action. Notice that no learning is done at this point.\n",
    "\n",
    "The following cell describes this how the agent implements this concept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Agent\n",
    "\n",
    "def act(self, state, eps=0.):\n",
    "    \"\"\"Returns actions for given state per current policy.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        state (array_like): current state\n",
    "        eps (float): epsilon, for epsilon-greedy action selection. \n",
    "        \n",
    "    Note\n",
    "    =====\n",
    "    For more information on what an epsilon-greedy action policy is, view the SARSA directory elsewhere in my github.\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert numpy to pyTorch tensor\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "    \n",
    "    # change the network to evaluation mode\n",
    "    self.qnetwork_local.eval()\n",
    "    \n",
    "    # disable gradient calculation in autograd. When disabled, autograd does not store the actions performed\n",
    "    # on the forward pass (which are used for gradient calculation with backward() is called).\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # obtain action values for this state produced by the network\n",
    "        action_values = self.qnetwork_local(state)\n",
    "        \n",
    "    # change mode to training\n",
    "    self.qnetwork_local.train()\n",
    "\n",
    "    # Epsilon-greedy action selection\n",
    "    if random.random() > eps:\n",
    "        return np.argmax(action_values.cpu().data.numpy())\n",
    "    else:\n",
    "        return random.choice(np.arange(self.action_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After receiving the action produced by the Q-Network, the Agent will progress the environment by calling *env.step(action)*. \n",
    "\n",
    "This call will return a new state vector that represents the updated environment, as well as a reward for performing that action.\n",
    "\n",
    "The agent will use this State, Action, Reward, Next_State experience to add to its Replay Buffer. In addition, the Agent will \"learn\" - update Q-Network weights - if the interval defined by 'update_rate' has been hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Agent\n",
    "\n",
    "def step(self, state, action, reward, next_state, done):\n",
    "    \n",
    "    \n",
    "    # for PER, this function works to populate the memory bank of PER.\n",
    "    # for uniform Experience Replay, comment out the line for memory_per.store and uncomment everything else.\n",
    "    \n",
    "    # Save experience in replay memory\n",
    "    #self.memory.add(state, action, reward, next_state, done)\n",
    "    \n",
    "    # grab states to fill the sumtree\n",
    "    self.memory_per.store((state, action, reward, next_state, done))\n",
    "    \n",
    "    # Learn every 'update_rate' time steps.\n",
    "    #self.t_step = (self.t_step + 1) % self.update_rate\n",
    "\n",
    "    #if self.t_step == 0:\n",
    "\n",
    "        # If enough samples are available in memory, get random subset and learn\n",
    "        #if len(self.memory) > self.batch_size:\n",
    "\n",
    "            #experiences = self.memory.sample()\n",
    "            #self.learn(experiences, self.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the learning interval has been reached, the Agent will update weights. The following cell implements the concepts discussed above under the section *Replay Experience* and *Decoupling*. In essence, keeps one set of weights fixed during N number of iterations allows a decoupling of the variable weights in the network and the target Q-value that these weights are attempting to approximate. By decoupling these values, the network does not introduce a moving target. \n",
    "\n",
    "For more information on this concept, please read the paper listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.Double Q-Network\n",
    "\n",
    "Hasselt, et al. (2015) introduced Double Deep Q-Learning Network that aims to address the overestimation present in standard Q-Learning and Deep Q-Network, such as that seen in my implementation of the DQN algorithm in this Github. \n",
    "\n",
    "**The following learn() function from the DQN has been modified to incorporate the Double DQN approach.**\n",
    "\n",
    "In this approach, the local network identifies the action (a') that results in the max q-value for a next state (s').\n",
    "Then, the target network obtains this action and finds the maximum state-action (s', a') q-value. This Q(s',a') value is used to calculate the TD error that the network is trained on. \n",
    "\n",
    "By decoupling the action selection from the max state-action q-value, the network is shown to train faster and perform more effectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Agent\n",
    "def learn(self, experiences, gamma):\n",
    "    \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "        gamma (float): discount factor\n",
    "    \"\"\"\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    \n",
    "    # Obtain the local-network predicted q-values for actions taken on the next_state.\n",
    "        \n",
    "    # change local network to inference mode. The aim here is to have the local network predict q-values for actions, not\n",
    "    # to learn.\n",
    "    self.qnetwork_local.eval()\n",
    "    \n",
    "    # disable gradient calculation in autograd. When disabled, autograd does not store the actions performed\n",
    "    # on the forward pass (which are used for gradient calculation with backward() is called).\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        \"\"\"\n",
    "         obtain action values for this state produced by the network.\n",
    "         the resulting vector is a (bs x 1) in shape, were bs is the batch size.\n",
    "         the scalar values are integers representing the action that produced the maximum q-value.\n",
    "         The argmax() call isolates 1st dimesion and returns the index corresponding to the largest value.\n",
    "         In this case, the 1st dimension is an array of q-values for all actions in the action space.\n",
    "        \"\"\"\n",
    "        next_state_action_values_localNet = self.qnetwork_local(next_states).argmax(1).unsqueeze(1)\n",
    "        \n",
    "        \n",
    "    # change mode to training\n",
    "    self.qnetwork_local.train()\n",
    "    \n",
    "    \"\"\"\n",
    "     Obtain the target-network predicted q-values for actions taken on the next_state.\n",
    "     detach() will produce a new tensor that is separated from the network. \n",
    "     gather() will extract the 1st dimension of the tensor, which correspond to the arrays of action values. \n",
    "     next_state_action_values_localNet identifies what index to obtain from the array of action values.\n",
    "     Again, this index corresponds to the action with the maximum estimated q-value by the local network.\n",
    "    \"\"\"\n",
    "    Q_targets_next = self.qnetwork_target(next_states).detach().gather(1, next_state_action_values_localNet)\n",
    "    \n",
    "    \"\"\"\n",
    "     Compute Q targets for current states. \n",
    "     When done[i] equals 1, the Q_target is the reward. This is consistent behavior with reinforcement learning applications\n",
    "     for the end state.\n",
    "    \n",
    "     At this point, the implementation of DDQN is complete, as the decoupled Q(s,a) has been obtained. \n",
    "    \"\"\"\n",
    "    Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "\n",
    "    # Get expected Q values from local model\n",
    "    \n",
    "    \"\"\"\n",
    "    qnetwork_local(states) returns a (bs x action_size) shaped tensor. For example, result.shape = 64x4, where result[0]\n",
    "    returns an array of length 4 corresponding to the various actions that can be taken in this environment and their\n",
    "    associated q-value. that is, result[0][0] = -0.47 for action ==> 0 ==> 'boost right'.\n",
    "    \n",
    "    gather(1, will operate on the 1st dimension, which corresponds to the arrays themselves.\n",
    "    'actions' is a (bs x 1) shaped vector (vertical vector), where each entry corresponds to the action that was taken. \n",
    "    that is, actions[5] = 3 for action 'boost left'. \n",
    "    gather(1,actions) will filter on the first dimension and pick the index of the array given by the corresponding entry\n",
    "    in the actions vector. this will return a (bs x 1) shaped vector where the scalar value is \n",
    "    the q-value of the action that was taken.\n",
    "    \n",
    "    Notice that detach is not called. so, autograd will record the operations performed in order to calculate gradients.\n",
    "    \"\"\"\n",
    "    Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = F.mse_loss(Q_expected, Q_targets)\n",
    "    \n",
    "    # Minimize the loss\n",
    "    # zero out the autograd so that prior gradient calculations are not incorporated into this pass.\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "    # update TARGET network by setting new weight values within the network.\n",
    "    self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Agent\n",
    "\n",
    "def learn_step(self, state, action, reward, next_state, done):    \n",
    "    \n",
    "    # store the action in PER memeory\n",
    "    self.memory_per.store((state, action, reward, next_state, done))\n",
    "    \n",
    "    # grab experiences, prioritized by TD error\n",
    "    tree_idx, batch, ISWeights_mb = self.memory_per.sample(self.batch_size)\n",
    "    #print(batch)\n",
    "    \n",
    "    #print('parsing data')\n",
    "    #for exp in batch:\n",
    "        #print(exp)\n",
    "    # parse data\n",
    "    #states = torch.from_numpy(np.vstack(np.array([each[0][0] for each in batch], ndmin=3))).float().to(device)\n",
    "    states = torch.from_numpy(np.vstack([exp.state for exp in batch if exp is not None])).float().to(device)\n",
    "    \n",
    "    #actions = torch.from_numpy(np.vstack(np.array([each[0][1] for each in batch]))).long().to(device)\n",
    "    actions = torch.from_numpy(np.vstack([exp.action for exp in batch if exp is not None])).long().to(device)\n",
    "    \n",
    "    #rewards = torch.from_numpy(np.vstack(np.array([each[0][2] for each in batch]) )).float().to(device)\n",
    "    rewards = torch.from_numpy(np.vstack([exp.reward for exp in batch if exp is not None])).float().to(device)\n",
    "    \n",
    "    #next_states = torch.from_numpy(np.vstack(np.array([each[0][3] for each in batch], ndmin=3))).float().to(device)\n",
    "    next_states = torch.from_numpy(np.vstack([exp.next_state for exp in batch if exp is not None])).float().to(device)\n",
    "    \n",
    "    #dones = torch.from_numpy(np.vstack(np.array([each[0][4] for each in batch])).astype(np.uint8)).float().to(device)\n",
    "    dones = torch.from_numpy(np.vstack([exp.done for exp in batch if exp is not None]).astype(np.uint8)).float().to(device)\n",
    "    \n",
    "    # turn local network into inference mode\n",
    "    self.qnetwork_local.eval()\n",
    "    \n",
    "    # turn on autograd\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # predict action values for next states using local network\n",
    "        next_state_action_values_localNet = self.qnetwork_local(next_states).argmax(1).unsqueeze(1)\n",
    "        \n",
    "    # reactivate training mode\n",
    "    self.qnetwork_local.train()\n",
    "    \n",
    "    # obtain TD Target using actions predicted from local network\n",
    "    Q_targets_next = self.qnetwork_target(next_states).detach().gather(1, next_state_action_values_localNet)\n",
    "    \n",
    "    # calculate TD Target\n",
    "    Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "    # calculate Q-value for current states\n",
    "    Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "    \n",
    "    abs_error_per = (Q_targets - Q_expected.detach()).squeeze().abs().cpu().numpy()\n",
    "    \n",
    "    self.memory_per.batch_update(tree_idx, abs_error_per)\n",
    "    \n",
    "    loss = (torch.from_numpy(ISWeights_mb).to(device) * ((Q_targets - Q_expected) **2)).mean()\n",
    "    \n",
    "    # zero out gradient calc. Want fresh pass\n",
    "    self.optimizer.zero_grad()\n",
    "    \n",
    "    # calc gradients for loss function in target network\n",
    "    loss.backward()\n",
    "    \n",
    "    # stop through optimizer\n",
    "    self.optimizer.step()    \n",
    "        \n",
    "    # Learn every 'update_rate' time steps.\n",
    "    self.t_step = (self.t_step + 1) % self.update_rate\n",
    "    #self.t_step = (self.t_step + 1) % 20\n",
    "\n",
    "    if self.t_step == 0:\n",
    "        \n",
    "        # update TARGET network by setting new weight values within the network\n",
    "        #print('updating network')\n",
    "        #print()\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau) \n",
    "    \n",
    "    #self.t_step_per += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A soft update is performed when updating the values of the target network. Tau specifies the degree to which the new value reflects the original value. This is seen by the multiplication of (tau) and (1-tau) in the update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Agent\n",
    "\n",
    "def soft_update(self, local_model, target_model, tau):\n",
    "    \"\"\"Soft update model parameters.\n",
    "\n",
    "    Î¸_target = Ï*Î¸_local + (1 - Ï)*Î¸_target\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        local_model (PyTorch model): weights will be copied from\n",
    "        target_model (PyTorch model): weights will be copied to\n",
    "        tau (float): interpolation parameter \n",
    "    \"\"\"\n",
    "\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "\n",
    "        # the 'underline' character indicates that this action is to be performed on the variable itself,\n",
    "        # rather than on a copy of the variable.\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  8. Train the Agent\n",
    "\n",
    "At this point, all the pieces are present to begin training the Agent in the Banana Collector environment using a Deep Q-Network.\n",
    "\n",
    "The following cell is very similar in structure to the function seen elsewhere in my github (for Discretization and SARSA) that interacts with the OpenAI Gym environment and the Agent. As such, I won't explain much here, as that has been done in other notebooks within my Github under Reinforcement Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 66\tAverage Score: -0.17Switching to PER\n",
      "Episode 100\tAverage Score: -0.01\n",
      "Episode 200\tAverage Score: 1.441\n",
      "Episode 300\tAverage Score: 2.66\n",
      "Episode 400\tAverage Score: 3.52\n",
      "Episode 500\tAverage Score: 5.13\n",
      "Episode 600\tAverage Score: 7.49\n",
      "Episode 700\tAverage Score: 7.49\n",
      "Episode 800\tAverage Score: 6.60\n",
      "Episode 900\tAverage Score: 5.34\n",
      "Episode 1000\tAverage Score: 5.98\n",
      "Episode 1100\tAverage Score: 10.34\n",
      "Episode 1200\tAverage Score: 3.792\n",
      "Episode 1300\tAverage Score: 7.13\n",
      "Episode 1400\tAverage Score: 10.16\n",
      "Episode 1500\tAverage Score: 11.08\n",
      "Episode 1600\tAverage Score: 11.11\n",
      "Episode 1700\tAverage Score: 11.16\n",
      "Episode 1800\tAverage Score: 10.62\n",
      "Episode 1900\tAverage Score: 10.67\n",
      "Episode 2000\tAverage Score: 10.82\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def deep_Q_learning(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"\n",
    "    Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    \n",
    "    total_count = 0\n",
    "    first_PER = True\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            \n",
    "            action = int(agent.act(state, eps))\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            \n",
    "            action = int(agent.act(state, eps))\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            \n",
    "            # populate the memory bank with no-ops.\n",
    "            # this is for PER. without PER, remove the if-else statement and just\n",
    "            # call agent.step(state,action,reward,next_state,done)\n",
    "            if total_count < 10000:\n",
    "                agent.step(state, action, reward, next_state, done)\n",
    "                total_count += 1\n",
    "            else:\n",
    "                if first_PER:\n",
    "                    print('Switching to PER')\n",
    "                    first_PER = False\n",
    "                agent.learn_step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                break \n",
    "        \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon. For more information on the importance of epsilon \n",
    "                                          # to an agent's action policy, see the SARSA directory in this Github.\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            \n",
    "        if np.mean(scores_window)>=15.0:\n",
    "            \n",
    "            # a mean score >= 13 represents a solved state. At this point, training has been complete.\n",
    "            # save the network weights for future testing.\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'banana_collect_ddqn.pth')\n",
    "            break\n",
    "            \n",
    "    return scores\n",
    "\n",
    "agent = Agent(state_size=37, action_size=4, seed=0)\n",
    "scores = deep_Q_learning()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Watch the Smart Agent Collect Bananas!\n",
    "\n",
    "The following code will utilize the trained network to successfully collect yellow bananas, while avoiding blue bananas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnityEnvironmentException",
     "evalue": "No Unity environment is loaded.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnityEnvironmentException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0b6bef2ac30d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m             \u001b[1;31m# reset the environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m                        \u001b[1;31m# get the current state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m                                                      \u001b[1;31m# initialize the score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0magent_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m37\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m       \u001b[1;31m# initialize smart agent and load trained weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0magent_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'banana_collect_ddqn.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\drlnd\\lib\\site-packages\\unityagents\\environment.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, train_mode, config, lesson)\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mUnityEnvironmentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No Unity environment is loaded.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mvector_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAllBrainInfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnityEnvironmentException\u001b[0m: No Unity environment is loaded."
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]             # reset the environment\n",
    "state = env_info.vector_observations[0]                        # get the current state\n",
    "score = 0                                                      # initialize the score\n",
    "agent_test = Agent(state_size=37, action_size=4, seed=0)       # initialize smart agent and load trained weights\n",
    "agent_test.qnetwork_local.load_state_dict(torch.load('banana_collect_ddqn.pth'))\n",
    "\n",
    "while True:\n",
    "    \n",
    "    action = int(agent_test.act(state))        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
