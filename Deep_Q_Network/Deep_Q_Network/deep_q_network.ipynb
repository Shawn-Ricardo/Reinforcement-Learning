{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network Applied to OpenAI Gym's Lunar Lander\n",
    "\n",
    "This notebook will implement a variation of the Deep Q-Network described in DeepMind's [Human-level Control Through Deep Reinforcement Learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf).\n",
    "\n",
    "\n",
    "### Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#!python -m pip install jdc\n",
    "import jdc\n",
    "\n",
    "#!python -m pip install pyvirtualdisplay\n",
    "#from pyvirtualdisplay import Display\n",
    "#display = Display(visible=0, size=(1400, 900)).start()\n",
    "#display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Environment\n",
    "\n",
    "Instantiate the [Lunar Lander Environment](https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py) and print the State and Action space.\n",
    "\n",
    "For more detailed description on the state vector and the environment in general, please visit the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if GPU available, take advantage of acceleration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Network\n",
    "\n",
    "The following cell implements the deep neural network that will be used to approximate the non-linear action-value function for this Lunar Lander environment. \n",
    "\n",
    "In the paper, the authors implemented a CNN because they defined the input space to be 84 x 84 x 4. This input space consisted of sequential images from the screen, allowing the agent to take advantage of spatial and temporal relationships that a CNN affords.\n",
    "\n",
    "In this approach, Lunder Lander does not return an image of a screen, but a 1-dimentional state vector. As such, a fully connected neural network will be utilized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        # fully connected layers. Notice how weight parameters are not being given. I am relying on PyTorch to \n",
    "        # initialize these for me.\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        # output layer\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        # utilize rectified linear units as activation functions for each neuron\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer\n",
    "\n",
    "Mnih et al. implemented a Replay Buffer to address several issues present with standard online Q-Learning applied to this environment:\n",
    "\n",
    "1) Each step of experience is potentially used in several update steps. This experience tuple can be a rare occurance/edge case and thus the agent can learn from these cases more often without having to rely on the hope of experiencing the rare occurance in the environment. \n",
    "\n",
    "2) There are strong correlations between consequtive frames in this environment. That is to say, an action at time step T will affect the state at timestep T+1, which in turn affects the state at timestep T+2, and so on. An agent that relies solely on sequential learning runs the risk of being stuck in a local minimum or divering catastrophically. \n",
    "\n",
    "### Decoupling \n",
    "\n",
    "3) When learning sequentially and on-policy (weights are updating every iteration), the current parameters determine the next data sample that these same parameters are trained on. This is the literal equivalent of aiming at a moving target. Replay Buffers allow a decoupling between the target and the parameters that are being changed to obtain this target when weights are kept fixed over N number of iterations. This concept is known as a Fixed Q-Target.\n",
    "\n",
    "In sum, having a replay buffer than contains prior experiences and learns from these experiences only after a certain number of iterations addresses the most salient issues present when adapting Q-Learning for this application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer. keep N number of the most recent experiences.\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size        \n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   The Agent \n",
    "\n",
    "The following cells define the Agent class, which implements the salient features of the above paper. These include Experience Replay and Fixed Q-Targets. For more information on these concepts, please refer to the paper. Specifically, the 'Methods' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed=0, buffer_size=int(1e5), \n",
    "                 batch_size=64, gamma=0.99, tau=1e-3, learning_rate=5e-4, update_rate=4):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "            buffer_size(int): reply buffer size\n",
    "            batch_size(int): minibatch size\n",
    "            gamma(float): discount factor\n",
    "            tau(float): utilized in the soft update of target parameters\n",
    "            lr(float): learning rate\n",
    "            update_rate(int): defines how often the network weights are updated\n",
    "        \"\"\"\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.learning_rate = learning_rate\n",
    "        self.update_rate = update_rate\n",
    "        \n",
    "        # Q-Network. As described in \"decoupling\" above, weights must be kept frozen for a certain number of iterations.\n",
    "        # This interval is defined by the paramters \"update_rate\". Q-Network implementation uses two networks to \"freeze\"\n",
    "        # weights while new experiences are collected.\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed)\n",
    "        \n",
    "        # Initialize time step for updating every 'update_rate' frequency\n",
    "        self.t_step = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lundar Lander program provides a vector of the current state. The agent accepts this state and inputs the state into the Q-Network, which produces an action. Notice that no learning is done at this point.\n",
    "\n",
    "The following cell describes this how the agent implements this concept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Agent\n",
    "\n",
    "def act(self, state, eps=0.):\n",
    "    \"\"\"Returns actions for given state per current policy.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        state (array_like): current state\n",
    "        eps (float): epsilon, for epsilon-greedy action selection. \n",
    "        \n",
    "    Note\n",
    "    =====\n",
    "    For more information on what an epsilon-greedy action policy is, view the SARSA directory elsewhere in my github.\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert numpy to pyTorch tensor\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "    \n",
    "    # change the network to evaluation mode\n",
    "    self.qnetwork_local.eval()\n",
    "    \n",
    "    # disable gradient calculation in autograd. When disabled, autograd does not store the actions performed\n",
    "    # on the forward pass (which are used for gradient calculation with backward() is called).\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # obtain action values for this state produced by the network\n",
    "        action_values = self.qnetwork_local(state)\n",
    "        \n",
    "    # change mode to training\n",
    "    self.qnetwork_local.train()\n",
    "\n",
    "    # Epsilon-greedy action selection\n",
    "    if random.random() > eps:\n",
    "        return np.argmax(action_values.cpu().data.numpy())\n",
    "    else:\n",
    "        return random.choice(np.arange(self.action_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After receiving the action produced by the Q-Network, the Agent will progress the environment by calling *env.step(action)*. \n",
    "\n",
    "This call will return a new state vector that represents the updated environment, as well as a reward for performing that action.\n",
    "\n",
    "The agent will use this State, Action, Reward, Next_State experience to add to its Replay Buffer. In addition, the Agent will \"learn\" - update Q-Network weights - if the interval defined by 'update_rate' has been hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Agent\n",
    "\n",
    "def step(self, state, action, reward, next_state, done):\n",
    "        \n",
    "    # Save experience in replay memory\n",
    "    self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "    # Learn every 'update_rate' time steps.\n",
    "    self.t_step = (self.t_step + 1) % self.update_rate\n",
    "\n",
    "    if self.t_step == 0:\n",
    "\n",
    "        # If enough samples are available in memory, get random subset and learn\n",
    "        if len(self.memory) > self.batch_size:\n",
    "\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, self.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the learning interval has been reached, the Agent will update weights. The following cell implements the concepts discussed above under the section *Replay Experience* and *Decoupling*. In essence, keeps one set of weights fixed during N number of iterations allows a decoupling of the variable weights in the network and the target Q-value that these weights are attempting to approximate. By decoupling these values, the network does not introduce a moving target. \n",
    "\n",
    "For more information on this concept, please read the paper listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Agent\n",
    "def learn(self, experiences, gamma):\n",
    "    \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "        gamma (float): discount factor\n",
    "    \"\"\"\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "    # Get max predicted Q values (for next states) from target model. \n",
    "    # notice that this is the same call performed in the 'act()' function above, for the local Qnetwork,\n",
    "    # which has variable targets.\n",
    "    \n",
    "    # Qnetwork_target has fixed weights that are kept constant through every update interval. \n",
    "    \n",
    "    \"\"\"\n",
    "    Quick Explanation of Following Line of Code:\n",
    "    \n",
    "     qnetwork_target(next_states) returns a (bs x action_size) shaped tensor, where 'bs' is the batch_size.\n",
    "     For example, if bs= = 64 and the action_space is 4, the returned tensor is 64x4, where the scalar values\n",
    "     (that is, result[row][action] = scalar_value) represent the q_values for each action.\n",
    "     detach() will return a new tensor detached from the network that will not require a gradient.\n",
    "     max(1)[0] will find the maximum along the 1st dimension, which is an array of q_values corresponding to the \n",
    "     actions that can be taken in the environment, and [0] will pick only the maximum value in the array. That is,\n",
    "     when portion max(1)[0] is fininsed, a tensor of size [64] (horizontal vector) will result, where each value is the \n",
    "     maximum q-value for that state. lastly, unsqueeze(1) creates the shape to [64,1] (or, a vertical vector).\n",
    "    \n",
    "    \"\"\"\n",
    "    Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "    \n",
    "    # Compute Q targets for current states. \n",
    "    # if done[i] equals 1, then Q_target is the reward. This is consistent behavior with reinforcement learning applications\n",
    "    # for the end state.\n",
    "    Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "    # Get expected Q values from local model\n",
    "    \n",
    "    \"\"\"\n",
    "    qnetwork_local(states) returns a (bs x action_size) shaped tensor. For example, result.shape = 64x4, where result[0]\n",
    "    returns an array of length 4 corresponding to the various actions that can be taken in this environment and their\n",
    "    associated q-value. that is, result[0][0] = -0.47 for action ==> 0 ==> 'boost right'.\n",
    "    \n",
    "    gather(1, will operate on the 1st dimension, which corresponds to the arrays themselves.\n",
    "    'actions' is a (bs x 1) shaped vector (vertical vector), where each entry corresponds to the action that was taken. \n",
    "    that is, actions[5] = 3 for action 'boost left'. \n",
    "    gather(1,actions) will filter on the first dimension and pick the index of the array given by the corresponding entry\n",
    "    in the actions vector. this will return a (bs x 1) shaped vector where the scalar value is \n",
    "    the q-value of the action that was taken.\n",
    "    \n",
    "    Notice that detach is not called. so, autograd will record the operations performed in order to calculate gradients.\n",
    "    \"\"\"\n",
    "    Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = F.mse_loss(Q_expected, Q_targets)\n",
    "    \n",
    "    # Minimize the loss\n",
    "    # zero out the autograd so that prior gradient calculations are not incorporated into this pass.\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "    # update TARGET network by setting new weight values within the network.\n",
    "    self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A soft update is performed when updating the values of the target network. Tau specifies the degree to which the new value reflects the original value. This is seen by the multiplication of (tau) and (1-tau) in the update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Agent\n",
    "\n",
    "def soft_update(self, local_model, target_model, tau):\n",
    "    \"\"\"Soft update model parameters.\n",
    "\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        local_model (PyTorch model): weights will be copied from\n",
    "        target_model (PyTorch model): weights will be copied to\n",
    "        tau (float): interpolation parameter \n",
    "    \"\"\"\n",
    "\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "\n",
    "        # the 'underline' character indicates that this action is to be performed on the variable itself,\n",
    "        # rather than on a copy of the variable.\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Train the Agent\n",
    "\n",
    "At this point, all the pieces are present to begin training the Agent on the Lundar Landing environment using a Deep Q-Network.\n",
    "\n",
    "The following cell is very similar in structure to the function seen elsewhere in my github (for Discretization and SARSA) that interacts with the OpenAI Gym environment and the Agent. As such, I won't explain much here, as that has been done in other notebooks within my Github under Reinforcement Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_Q_learning(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"\n",
    "    Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            \n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                break \n",
    "                \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon. For more information on the importance of epsilon \n",
    "                                          # to an agent's action policy, see the SARSA directory in this Github.\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            \n",
    "        if np.mean(scores_window)>=200.0:\n",
    "            \n",
    "            # a mean score >= 200 represents a solved state. At this point, training has been complete.\n",
    "            # save the network weights for future testing.\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "            \n",
    "    return scores\n",
    "\n",
    "agent = Agent(state_size=8, action_size=4, seed=0)\n",
    "scores = deep_Q_learning()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch the Smart Agent Land the Spaceship!\n",
    "\n",
    "The following code will utilize the trained network to successfully land the spaceship within the goal region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights from file\n",
    "agent_test = Agent(state_size=8, action_size=4, seed=0)\n",
    "agent_test.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "for i in range(3):\n",
    "    state = env.reset()\n",
    "    img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    for j in range(600):\n",
    "        action = agent_test.act(state)\n",
    "        img.set_data(env.render(mode='rgb_array')) \n",
    "        plt.axis('off')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
