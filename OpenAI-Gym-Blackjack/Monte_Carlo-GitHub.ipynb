{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of OpenAI Gym Blackjack environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each state is a 3-tuple of:\n",
    "- the player's current sum $\\in \\{0, 1, \\ldots, 31\\}$,\n",
    "- the dealer's face up card $\\in \\{1, \\ldots, 10\\}$, and\n",
    "- whether or not the player has a usable ace (`no` $=0$, `yes` $=1$).\n",
    "\n",
    "The agent has two potential actions:\n",
    "\n",
    "```\n",
    "    STICK = 0\n",
    "    HIT = 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_from_Q(env, Q, epsilon, nA):\n",
    "    \n",
    "    # 'episode' is a list that holds the [[state], [action], reward]\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # 'action' is obtained by implementing a stoichastic policy based on the greedily obtained Q-Table.\n",
    "        # get_probs() will return a list with as many entries as possible actions in this environment.\n",
    "        # in this case, nA = 2.\n",
    "        # associated with that list are probabilities of choosing that particular index. \n",
    "        # for example, p=[0.33, 0.66], which indicates that there is a 0.33 probability to choose index 0\n",
    "        # and a 0.66 probability of choosing index 1.\n",
    "        # numpy will then take this information and return either 0 or 1, which corresponds to a stick(0) or \n",
    "        # a hit(1). \n",
    "        # Note: this only occurs if the Q-Table has an entry for the given state, denoted by\n",
    "        # 'if state in Q'\n",
    "        # else, an action is chosen at random.\n",
    "        action = np.random.choice(np.arange(nA), p=get_probs(Q[state], epsilon, nA)) \\\n",
    "                                    if state in Q else env.action_space.sample()\n",
    "\n",
    "        # play out the hand given the action. \n",
    "        # note that 'done' can be True due to the agent choosing to stay and winning/losing, from the agent \n",
    "        # choosing to hit and going over 21, or from the agent choosing to hit/stay and drawing. \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # append the state, action, and reward to the episode list.\n",
    "        # note that the state is the prior state, not the next_state after having performed the action.\n",
    "        # this piece of code wraps up the state, the action, and the result of that action.\n",
    "        episode.append((state, action, reward))\n",
    "        \n",
    "        # advance the state.\n",
    "        state = next_state\n",
    "        \n",
    "        # if the hand is over, exit the loop.\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    # return the results of playing several hands of blackjack\n",
    "    return episode\n",
    "\n",
    "def get_probs(Q_s, epsilon, nA):\n",
    "    \n",
    "    # initialize a list with as many indices as there are actions in this environment.\n",
    "    # for the case of blackjack, nA = 2\n",
    "    # every entry is initialized with the same probability, epsilon / nA\n",
    "    policy_s = np.ones(nA) * epsilon / nA\n",
    "    \n",
    "    # Q_s is a list of size 2 that contains expected rewards for a particular action.\n",
    "    # e.g., Q_s = [0.87, -0.44]\n",
    "    # This indicates that sticking(index 0) has an expected reward of 0.87 for this particular state.\n",
    "    # on the other hand, hitting(index 1) has an expected reward of -0.44.\n",
    "    # best_a will contain the INDEX of the largest value in the list.\n",
    "    best_a = np.argmax(Q_s)\n",
    "    \n",
    "    # This line will increase the probability of choosing the action that corresponds to the greater\n",
    "    # expected reward.\n",
    "    # Note that we add to the equiprobably value of (epsilon/nA) an amount of (1-epsilon).\n",
    "    # Early in training, epsilon is closer to 1, which results in a smaller increment to\n",
    "    # the currently highest rewarded action.\n",
    "    # This results in other actions have a greater chance of being executed, and thus the agent\n",
    "    # will 'explore' rather than exploit.\n",
    "    # On the other hand, as training increases, epsilon becomes smaller and the value (1-epsilon) grows\n",
    "    # which increases the probability of choosing the greater expected reward -- i.e., the agent favors the\n",
    "    # exploitation of its knowledge of the environment. \n",
    "    policy_s[best_a] = 1 - epsilon + (epsilon / nA)\n",
    "    \n",
    "    # summary: in the beginning of training, the policy may look like\n",
    "    # [0.48, 0.44]\n",
    "    # for a particular state.\n",
    "    # toward the end of training, the policy may look like\n",
    "    # [0.56, -0.81]\n",
    "    return policy_s\n",
    "\n",
    "def update_Q(env, episode, Q, alpha, gamma):\n",
    "    \n",
    "    # place the states, actions, and rewards into separate lists\n",
    "    states, actions, rewards = zip(*episode)\n",
    "    \n",
    "    # initialize a list that contains discounted expected rewards.\n",
    "    # discounting is not implemented here, but will be discussed/explained in more detail \n",
    "    # in later projects in this repo.\n",
    "    discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "    \n",
    "    # this loop will update the Q-Table. \n",
    "    for i, state in enumerate(states):\n",
    "        \n",
    "        # get the current value in the for a given State/Action pair\n",
    "        old_Q = Q[state][actions[i]] \n",
    "        \n",
    "        # update the Q-Table using a variable alpha. \n",
    "        # alpha corresponds to the amount of influence the new reward has in updating the current Q-Table entry.\n",
    "        # if alpha is large, then the new expected reward has a greater effect on the value in the Q-Table.\n",
    "        # conversely, if alpha is small, then the new reward has a smaller impact on the value in the Q-Table.\n",
    "        Q[state][actions[i]] = old_Q + alpha*(sum(rewards[i:]*discounts[:-(1+i)]) - old_Q)\n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control(env, num_episodes, alpha, gamma=1.0, eps_start=1.0, eps_decay=.99999, eps_min=0.05):\n",
    "\n",
    "    # number of actions in the action space = 2, for 'stick' and 'hit'\n",
    "    nA = env.action_space.n\n",
    "    \n",
    "    # Q-Table will hold the expected reward for a given State/Action pair. It is a nexted dictionary.\n",
    "    # Q[state][action] = reward\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    \n",
    "    # The value of epsilon determines the degree to which the Agent trusts its knowledge of the environment\n",
    "    # when determining its next action. Early on, it is favorable for the Agent to explore various \n",
    "    # State/Action spaces to gain a more well-rounded understanding of its environment.\n",
    "    epsilon = eps_start\n",
    "\n",
    "    # num_episodes is equivalent to the number of hands of Blackjack that will be played.\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        \n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        # set the value of epsilon. Note the decay rate and that there is a limit to the minimum of \n",
    "        # epsilon. If epsilon becomes too low too fast, the agent will strictly favor exploitation.\n",
    "        epsilon = max(epsilon*eps_decay, eps_min)\n",
    "        \n",
    "        # generate a list of episodes by following epsilon-greedy policy\n",
    "        episode = generate_episode_from_Q(env, Q, epsilon, nA)\n",
    "        \n",
    "        # update the action-value function estimate using the episode\n",
    "        Q = update_Q(env, episode, Q, alpha, gamma)\n",
    "        \n",
    "    # determine the policy corresponding to the final action-value function estimate.\n",
    "    # 'policy' will contain a dictionary of States and Actions, \n",
    "    # e.g., [[13, 5, False], 1] --> given the agent has a sum of 13, the dealer is showing a card of value 5,\n",
    "    # and the agent does not have a usable Ace, the best action is to hit.\n",
    "    policy = dict((k,np.argmax(v)) for k, v in Q.items())\n",
    "    \n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a policy and Q-Table. 500,000 = number of episodes to play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500000/500000."
     ]
    }
   ],
   "source": [
    "policy, Q = mc_control(env, 500000, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
