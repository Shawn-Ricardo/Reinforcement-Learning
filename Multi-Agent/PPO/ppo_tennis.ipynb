{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Proximal Policy Optimization\n",
    "\n",
    "\n",
    "This notebook implements Proximal Policy Optimization and describes each detail in step. Please refer to the readme to gain a deeper understanding of the conceptual peices of the paper. \n",
    "\n",
    "The environment to solve is Unity's [Tennis Environment](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md). Please refer to the readme or the link for more information. In general, to achieve an average score of 0.5 is considered \"solving\" the environment.\n",
    "\n",
    "### Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pdb\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from unityagents import UnityEnvironment\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Policy\n",
    "\n",
    "PPO implements an actor-critic architecture to estimate the action policy and state-value function. Please refer to the readme for more details.\n",
    "\n",
    "The following cells were modified/inspired from:\n",
    "[Olivier St-Amand](https://github.com/ostamand), [Lilian Weng](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#maddpg), [Thomas Simonini](https://towardsdatascience.com/proximal-policy-optimization-ppo-with-sonic-the-hedgehog-2-and-3-c9c21dbed5e), [Shangtong Zhang](https://github.com/ShangtongZhang/DeepRL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, hidden_size=[512, 256]):\n",
    "        \n",
    "        \"\"\" \n",
    "        \n",
    "        initialize the ActorCritic class\n",
    "        \n",
    "        params:\n",
    "            state_size   -  size of the observation space vector\n",
    "            action_size  -  size of the action space vector\n",
    "            hidden_size  -  list containing the input/output dimensions of\n",
    "                            the hidden layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # two neural network layers. These layer will \n",
    "        # be used by both the actor and critic initially.\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size[0])\n",
    "        self.fc2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        \n",
    "        # Actor. output the continuous values for the action vector.\n",
    "        self.actor = nn.Linear(hidden_size[1], action_size)\n",
    "        \n",
    "        # Interestly, Olivier St-Amand uses a neural network to estimate the standard\n",
    "        # deviation of the action policy distribution. \n",
    "        # In most other implementations, this value is usually fixed to 1.\n",
    "        self.actor_std = nn.Linear(hidden_size[1], action_size)\n",
    "\n",
    "        # Critic. map state to a single value. \n",
    "        self.critic = nn.Linear(hidden_size[1], 1)\n",
    "\n",
    "    def forward(self, x, actions=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        forward pass through actor-critic network.\n",
    "        \n",
    "        Params:\n",
    "            x      - observation vector\n",
    "            action - vector of actions. used when calculating r(theta). \n",
    "                     See readme for detailed explanation of r(theta) and\n",
    "                     how it is used.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # move input 'x' through layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Actor. obtain the estimated action vector values.\n",
    "        # tanh returns values between -1 and 1, which is also \n",
    "        # the range of the Unity environment.\n",
    "        estimated_actions = torch.tanh(self.actor(x))\n",
    "        \n",
    "        # Olivier St-Amand uses a SOFTPLUS here. The softplus function outputs values \n",
    "        # within [0, +infinity]. Appropriate for a standard deviation.\n",
    "        action_distribution_std = F.softplus(self.actor_std(x))\n",
    "        \n",
    "        # using 'actions' as a mean and the deviations obtained above, create a Gaussian distribution\n",
    "        action_distribution = torch.distributions.Normal(estimated_actions, action_distribution_std)\n",
    "        \n",
    "        # if no actions where passed in, sample from this distribution to obtain\n",
    "        # a vector of action values.\n",
    "        if actions is None:\n",
    "            actions = action_distribution.sample()\n",
    "        \n",
    "        # obtain the log probability of the action policy. \n",
    "        # this is equation (2) from the paper.\n",
    "        log_probabilities = action_distribution.log_prob(actions)\n",
    "\n",
    "        # Critic.\n",
    "        # estimate the state value, V(s) -> v.\n",
    "        state_value_estimate = self.critic(x)\n",
    "\n",
    "        # the entropy of a distribution can be obtained directly through the entropy() \n",
    "        # method. This is useful for equation (9) from the paper, when including an\n",
    "        # \"entropy bonus\".\n",
    "        return actions, log_probabilities, action_distribution.entropy(), state_value_estimate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Agent\n",
    "\n",
    "The PPO Agent will use the actor-critic policy defined above to train itself in this Unity Environment. The PPO algorithm is implemented in the step() function. For a more detailed explanation, please view the readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PPOAgent():\n",
    "    \n",
    "    def __init__(self, env=None, \n",
    "                 bootstrap_length=2048,   epochs=10,   batch_size=32,   epsilon=0.1, \n",
    "                 lrate=1e-4,   lrate_schedule=lambda it: max(0.995 ** it, 0.01), \n",
    "                 entropy_coeff=0.00,   vf_coeff=1.0,   gae_tau=0.95, \n",
    "                 discount_factor=0.99,   gradient_clip=0.5):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        Initialize PPO Agent\n",
    "        \n",
    "        params:\n",
    "        \n",
    "            env                - unity environment\n",
    "            bootstrap_length   - number of iterations to populate trajectory\n",
    "            epochs             - number of iterations to perform SGD on policy\n",
    "            batch_size         - number of experiences to sample from trajectory\n",
    "            epsilon            - clip probability ratio by this amount\n",
    "            lrate              - learning rate for policy optimizer\n",
    "            lrate_schedule     - REMOVE\n",
    "            entropy_coeff      - degree to which the entropy bonus is incorporated into \n",
    "                                 surrogate. see equation (9)\n",
    "            vf_coeff           - degree to which the value function is incorporated into\n",
    "                                 surrogate. see equation (9)\n",
    "            gae_tau            - generalized advantage estimation parameter\n",
    "            discount_factor    - discount factor for future rewards\n",
    "            gradient_clip      - clipping parameter for policy gradient\n",
    "            \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # ensure environment is present.\n",
    "        assert(env != None)\n",
    "        \n",
    "        # set information pertaining to unity environment.\n",
    "        self.env = env\n",
    "        self.brain_name = env.brain_names[0]\n",
    "        self.brain = env.brains[self.brain_name]\n",
    "        self.env_info = self.env.reset(train_mode=True)[self.brain_name]\n",
    "        self.num_agents = len(self.env_info.agents)\n",
    "        self.action_size = self.brain.vector_action_space_size\n",
    "        self.state = self.env_info.vector_observations\n",
    "        self.state_size = self.state.shape[1]\n",
    "        \n",
    "        # if gpu is available, take advantage.\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        # define the policy to optimize.\n",
    "        self.policy = ActorCritic(self.state_size, self.action_size).to(self.device)\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.bootstrap_length = bootstrap_length\n",
    "        self.discount_factor = discount_factor\n",
    "        self.optimization_epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon\n",
    "        self.lrate = lrate\n",
    "        self.gradient_clip = gradient_clip\n",
    "        self.entropy_coeff = entropy_coeff\n",
    "        self.vf_coeff = vf_coeff\n",
    "        self.gae_tau = gae_tau\n",
    "        self.lrate_schedule = lrate_schedule\n",
    "        \n",
    "        # if gpu available, utilize\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # optimizer\n",
    "        self.opt = optim.Adam(self.policy.parameters(), lr=self.lrate)\n",
    "\n",
    "        # lrate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.LambdaLR(self.opt, lr_lambda=lrate_schedule)\n",
    "\n",
    "        self.rewards = np.zeros(self.num_agents)\n",
    "        self.episodes_reward = []\n",
    "        \n",
    "        # keep track of total steps taken in environment\n",
    "        self.steps = 0\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def running_lrate(self):\n",
    "        return self.opt.param_groups[0]['lr']\n",
    "    \n",
    "    \n",
    "    def close_env(self):\n",
    "        \"\"\" \n",
    "        \n",
    "        terminate the unity environment\n",
    "        \n",
    "        \"\"\"\n",
    "        self.env.close()\n",
    "        \n",
    "    def reset(self, is_training=True):\n",
    "        \"\"\" \n",
    "        \n",
    "        reset unity environment and return observation vector\n",
    "        \n",
    "        params:\n",
    "            is_training - boolean. place unity environment in training mode or not.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.env_info = self.env.reset(train_mode=is_training)[self.brain_name]\n",
    "        self.state = self.env_info.vector_observations\n",
    "        return self.state\n",
    "        \n",
    "    \n",
    "    def tensor_from_np(self, x):\n",
    "        \"\"\"\n",
    "        convert numpy array into pytorch tensor\n",
    "        \n",
    "        params:\n",
    "            x - numpy array\n",
    "        \"\"\"\n",
    "        return torch.from_numpy(x).float().to(self.device)\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_batch(self, states, actions, old_log_probs, returns, advs):\n",
    "        \"\"\"\n",
    "         yeild batches of experiences, chosen at random\n",
    "         \n",
    "         params:\n",
    "             states          -  array of observation space vectors\n",
    "             actions         -  array of action space vectors\n",
    "             old_log_probs   -  log probabilities of action distributions\n",
    "             returns         -  returns calculated from a trajectory\n",
    "             advs            -  advantages calculated from a trajectory\n",
    "        \"\"\"\n",
    "        \n",
    "        length = states.shape[0]\n",
    "        \n",
    "        batch_size = int(length / self.batch_size)\n",
    "        \n",
    "        idx = np.random.permutation(length)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            \n",
    "            rge = idx[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            yield (states[rge], actions[rge], old_log_probs[rge], returns[rge], advs[rge].squeeze(1))\n",
    "            \n",
    "\n",
    "    def step(self):\n",
    "        \n",
    "        # step lrate scheduler\n",
    "        self.scheduler.step()\n",
    "\n",
    "        # begin bootstrapping\n",
    "        \n",
    "        # holds all experience tuples from this bootstrap session.\n",
    "        trajectory_raw = []\n",
    "        \n",
    "        for _ in range(self.bootstrap_length):\n",
    "\n",
    "            # convert current state to tensor\n",
    "            state = self.tensor_from_np(self.state)\n",
    "            \n",
    "            # pass current state through policy to obtain estimated actions, log probabilities of these actions,\n",
    "            # and the critics estimation of the value of this state.\n",
    "            action, log_p, _, value = self.policy(state)\n",
    "\n",
    "            # detach variables\n",
    "            log_p = log_p.detach().cpu().numpy()\n",
    "            value = value.detach().squeeze(1).cpu().numpy()\n",
    "            action = action.detach().cpu().numpy()\n",
    "\n",
    "            # using the estimated actions, advance the environment\n",
    "            self.env_info = self.env.step(np.clip(action, -1, 1))[self.brain_name]\n",
    "            next_state = self.env_info.vector_observations\n",
    "            reward = np.array(self.env_info.rewards)\n",
    "            done = np.array(self.env_info.local_done)\n",
    "                        \n",
    "            # collect reward. this is a running total\n",
    "            self.rewards += reward\n",
    "\n",
    "            # check if some episodes are done\n",
    "            for i, d in enumerate(done):\n",
    "                \n",
    "                if d:\n",
    "                    # for this episode that has completed, set its final reward.  \n",
    "                    self.episodes_reward.append(self.rewards[i])\n",
    "                    \n",
    "                    # since we are no longer collecting rewards for this episode (have places into\n",
    "                    # 'episodes_reward'), set to 0.\n",
    "                    self.rewards[i] = 0\n",
    "\n",
    "            # add this trajectory to the list\n",
    "            trajectory_raw.append((state, action, reward, log_p, value, 1-done))\n",
    "            \n",
    "            \n",
    "            if np.any(done):\n",
    "                # reset state\n",
    "                self.state = self.reset()\n",
    "            else:\n",
    "                # advance state\n",
    "                self.state = next_state\n",
    "            \n",
    "            \n",
    "        # finished bootstrapping \n",
    "        # time to calculate advantages and returns of the trajectory. \n",
    "        # see the Algorithm 1 in paper. This portion is equivalent to\n",
    "        # \"Compute advantage estimates...\"\n",
    "        \n",
    "        # get the estimated value of the terminating state from critic\n",
    "        next_value = self.policy(self.tensor_from_np(self.state))[-1].detach().squeeze(1)\n",
    "        \n",
    "        # create final trajectory\n",
    "        trajectory_raw.append((state, None, None, None, next_value.cpu().numpy(), None))\n",
    "        trajectory = [None] * (len(trajectory_raw)-1)\n",
    "        \n",
    "        # process raw trajectories\n",
    "        # calculate advantages and returns\n",
    "        advs = torch.zeros(self.num_agents, 1).to(self.device)\n",
    "        \n",
    "        R = next_value\n",
    "\n",
    "        # from last to first\n",
    "        for i in reversed(range(len(trajectory_raw)-1)):\n",
    "\n",
    "            # grab trajectory_i\n",
    "            states, actions, rewards, log_probs, values, dones = trajectory_raw[i]\n",
    "            \n",
    "            # convert each of these into a tensor.\n",
    "            # the new variable is 'next_values', which is the successive trajectory values.\n",
    "            actions, rewards, dones, values, next_values, log_probs = map(\n",
    "                lambda x: torch.tensor(x).float().to(self.device),\n",
    "            (actions, rewards, dones, values, trajectory_raw[i+1][-2], log_probs))\n",
    "            \n",
    "            # calculate the return.\n",
    "            # if an epsiode terminated, dones[terminating-episode-index] == 0, so that reward is the return.\n",
    "            R = rewards + self.discount_factor * R * dones\n",
    "            \n",
    "            # at this point, Generalized ADVANTAGE Estimation (GAE) is implemented.\n",
    "            # GAE can be used with virtually any policy-based method, such as PPO here.\n",
    "            # https://arxiv.org/abs/1506.02438\n",
    "            \n",
    "            # without GAE, advantage is calculated as: adv = Returns - Values; see the \n",
    "            # readme for a detailed explanation.\n",
    "            td_errors = rewards + self.discount_factor * dones * next_values - values\n",
    "            advs = advs * self.gae_tau * self.discount_factor * dones[:, None] + td_errors[:, None]\n",
    "            \n",
    "            # insert the advantage into the list. These will be used to train the network.\n",
    "            trajectory[i] = (states, actions, log_probs, R, advs)\n",
    "\n",
    "        # for every (states, actions, log_probs, R, advs), map to corresponding variables. \n",
    "        states, actions, old_log_probs, returns, advs = map(lambda x: torch.cat(x, dim=0), zip(*trajectory))\n",
    "\n",
    "        # normalize advantages\n",
    "        advs = (advs - advs.mean())  / (advs.std())\n",
    "\n",
    "        \n",
    "        # train policy with random batchs of accumulated trajectories\n",
    "        for _ in range(self.optimization_epochs):\n",
    "\n",
    "            for states_b, actions_b, old_log_probs_b, returns_b, advs_b in \\\n",
    "                self.get_batch(states, actions, old_log_probs, returns, advs):\n",
    "\n",
    "                # get updated values from policy. Note at this point that actions are passed in, acting\n",
    "                # as the mean to a distribution and the resulting new log_probabilities are obtained. \n",
    "                _, new_log_probs_b, entropy_b, values_b = self.policy(states_b, actions_b)\n",
    "\n",
    "                # obtain the ratio. expo() will returns a new tensor \n",
    "                # with the exponential of the difference of the new and old log probabilties.\n",
    "                # this is r(theta) is equation (6) of the paper.\n",
    "                ratio = (new_log_probs_b - old_log_probs_b).exp()\n",
    "\n",
    "                # Clipped function, equation (7)\n",
    "                clip = torch.clamp(ratio, 1-self.epsilon, 1+self.epsilon)                \n",
    "                # implement equation (7) of the paper.\n",
    "                clipped_surrogate = torch.min(ratio*advs_b.unsqueeze(1), clip*advs_b.unsqueeze(1))\n",
    "\n",
    "                # actor loss.\n",
    "                actor_loss = -torch.mean(clipped_surrogate) - self.entropy_coeff * entropy_b.mean()\n",
    "                \n",
    "                # critic loss, according to equation (9) in the paper. \n",
    "                # in the paper, the critic loss hyper-parameter is 1.\n",
    "                critic_loss = F.smooth_l1_loss(values_b, returns_b.unsqueeze(1))\n",
    "\n",
    "                # remove any preexisting gradient\n",
    "                self.opt.zero_grad()\n",
    "                \n",
    "                # backprop surrogate\n",
    "                (actor_loss + critic_loss).backward()\n",
    "                \n",
    "                # clip the gradient values. stabilitizes learning\n",
    "                nn.utils.clip_grad_norm_(self.policy.parameters(), self.gradient_clip)\n",
    "                \n",
    "                # optimize policy\n",
    "                self.opt.step()\n",
    "\n",
    "        # steps of the environement processed by the agent \n",
    "        self.steps += self.bootstrap_length * self.num_agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(agent, iterations=500,  print_every=1,  policy_save_threshold=0.5):\n",
    "    \"\"\"\n",
    "    \n",
    "    train a PPO agent and visualize running rewards\n",
    "    \n",
    "    params:\n",
    "        iterations             -  amount to run the agent.step() method\n",
    "        print_every            -  the rate at which to log rewards\n",
    "        policy_save_threshold  - the reward value at which \n",
    "                                 to start saving the policy parameters\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # will hold all rewards\n",
    "    rewards = []\n",
    "    \n",
    "    # will hold the mean rewards from each iteration\n",
    "    mean_per_iteration = []\n",
    "    \n",
    "    for iter in range(iterations):\n",
    "        \n",
    "        # step agent. \n",
    "        agent.step()\n",
    "\n",
    "        # grab last 100 rewards from agent\n",
    "        if len(agent.episodes_reward) >= 100:\n",
    "            \n",
    "            # last 100 rewards\n",
    "            r = agent.episodes_reward[:-101:-1]\n",
    "            # append stats\n",
    "            rewards.append((agent.steps, min(r), max(r), np.mean(r), np.std(r)))\n",
    "\n",
    "        # check logging condition\n",
    "        if (iter+1) % print_every == 0:\n",
    "            \n",
    "            # variable to print\n",
    "            summary = ''\n",
    "            \n",
    "            if rewards:\n",
    "                \n",
    "                mean = rewards[-1][3]\n",
    "                minimum = rewards[-1][1]\n",
    "                maximum = rewards[-1][2]\n",
    "                summary = f'Rewards: {mean:.2f}/{rewards[-1][4]:.2f}/{minimum:.2f}/{maximum:.2f} (mean/std/min/max)'\n",
    "                mean_per_iteration.append(mean)\n",
    "\n",
    "                if mean >= policy_save_threshold:\n",
    "                    policy_save_threshold = mean\n",
    "                    torch.save(agent.policy.state_dict(), '.\\ppo_tennis_best.pth')\n",
    "                    print('\\ntarget reward obtained!')\n",
    "                    break\n",
    "                    \n",
    "            # clear prior output, print stats and plot learning curve.\n",
    "            clear_output(True)\n",
    "            print(f\"Iteration: {iter+1:d}\\nEpisodes: {len(agent.episodes_reward)}\\nSteps: {agent.steps:d}\\nlrate: {agent.running_lrate:.2E}\\n{summary}\", end=\"\")\n",
    "            plt.figure(figsize=(40,5))\n",
    "            plt.subplot(131)\n",
    "            plt.xlabel('iteration')\n",
    "            plt.ylabel('mean reward')\n",
    "            plt.plot(mean_per_iteration)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent\n",
    "\n",
    "Instantiate the environment and train the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 88\n",
      "Episodes: 10932\n",
      "Steps: 360448\n",
      "lrate: 6.47E-05\n",
      "Rewards: 0.46/0.55/-0.01/2.60 (mean/std/min/max)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target reward obtained!\n"
     ]
    }
   ],
   "source": [
    "# instantiate unity environment\n",
    "env = UnityEnvironment(file_name='Tennis.exe', no_graphics=False)  \n",
    "\n",
    "# instantiate agent\n",
    "agent = PPOAgent(env)\n",
    "\n",
    "# train the agent\n",
    "train(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch the Agents Play Tennis!\n",
    "\n",
    "Test the agent using the best performing policy parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 1.2450000187382102\n"
     ]
    }
   ],
   "source": [
    "# load best performing policy\n",
    "agent.policy.load_state_dict(torch.load('ppo_tennis_best.pth'))\n",
    "\n",
    "# reset state and set training mode to false\n",
    "state = agent.reset(is_training=False)\n",
    "\n",
    "scores = np.zeros(agent.num_agents)\n",
    "\n",
    "for i in range(500):\n",
    "    \n",
    "    state = agent.tensor_from_np(np.array(state))\n",
    "    action, _, _, _ = agent.policy(state)\n",
    "    action = action.detach().cpu().numpy()\n",
    "    action = np.clip(action, -1, 1)                                  # all actions between -1 and 1\n",
    "    env_info = agent.env.step(action)[agent.brain_name]    # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations                       # get next state (for each agent)\n",
    "    rewards = env_info.rewards                                       # get reward (for each agent)\n",
    "    dones = env_info.local_done                                      # see if episode finished\n",
    "    scores += env_info.rewards                                       # update the score (for each agent)\n",
    "    state = next_states                                              # roll over states to next time step        \n",
    "    if np.any(dones):                                                 # reset if episode finished\n",
    "        # reset state\n",
    "        state = agent.reset()\n",
    "    else:\n",
    "        # advance state\n",
    "        state = next_states\n",
    "        \n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the unity environment\n",
    "agent.close_env()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
